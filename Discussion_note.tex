\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\allowdisplaybreaks
\numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[theorem]
\newtheorem{remark}{Remark}[theorem]

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{graphicx}
\usepackage{float} 
\usepackage{subfigure}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\bibliographystyle{plain}
\allowdisplaybreaks[2]

\def\Argmin{\mathop{\mathrm{Argmin}}}


\title{Discussion Note}
\author{Zhang Hao}

\begin{document}
\maketitle


\section{Introduction }
Problems whose structure involves a separable objective function and variables subject to linear constraints are
common in the fields of statistics, machine learning, scientific computing, etc. 
A separable objective fucntion can be expressed as the sum (or product) of functions of individual variables 
or groups of variables. A significant benefit of these functions is that 
This separable structure allows the original problem to be decomposed into smaller, simpler subproblems that can be solved independently. 
Many objective functions of optimization problems are separable such as the sum of squared errors in linear regression, 
the sum of hinge losses in support vector machines and the sum of negative log-likelihoods in generalized linear models.

Besides, Linear constraints, which take the form of equalities involving linear combinations of the variables, 
are very common in optimization problems.
These constraints can represent some requirements produced in practical application background that the solution must satisfy. 
They can also occur during the variable replacement. 

A classic method to solve these problems with such structure is ADMM (Alternating Direction Method of Multipliers). 
In each iteration, ADMM needs to compute the projection. 
However, in some specific applications, the p
Therefore, in order to overcome this drawback, we can use other first-order
algorithms such as Frank-Wolfe method.

The Frank-Wolfe type method, also known as conditional gradient method, has become quite popular recently because it only requires to minimize the LO which is the linear approximation of the
objective function. Compared to computing the projection in proximal gradient method, solving LO can be much more
efficient in practice.

In this paper, we consider the following problem
\begin{equation}
    \begin{array}{cl} \label{problem}
        \min    &f(x)+ g(y) \\
        {\rm{s.t.}} &Ax+By=c
        \end{array}
\end{equation}
where $A: \mathcal{E}_1 \rightarrow \mathcal{E}$, $B: \mathcal{E}_2 \rightarrow \mathcal{E}$ are linear maps, 
$f: \mathcal{E}_1 \rightarrow \mathbb{R}\cup\{\infty\}$, 
$g: \mathcal{E}_2 \rightarrow \mathbb{R}\cup\{\infty\}$ are two proper closed convex functions 
and may not be smooth. In some application scenarios, it is easy to compute the projection onto $\mathrm{dom} f$. However, the projection onto $\mathrm{dom} g$ is not as straightforward. In contrast, 
it is easy to solve the LO of $g$. Therefore, we can get $x_{k+1}$ by proximal gradient method and get $y_{k+1}$ by Frank-Wolfe
method.  The details of algorithm is in Section \ref{section_convergence_Lipschitz}.

In this paper, we first consider the situation that $f$ and $g$ have Lipschitz continous gradient. In this case, 
we find that the function value $f(x^k)+ g(y^k)$ converges to the optimal value of problem \eqref{problem}
at the rate of $\mathcal{O}\left( \frac{1}{\sqrt{k}} \right)$. Additionally, the iteration points $\{x^k\}$, $\{y^k\}$ 
satisfy the property that $\|Ax^k+By^k-c\rVert$  converges to 0 at the rate of $\mathcal{O}\left(\frac{1}{\sqrt{k}}\right)$ 
under some moderate assumptions. 
Furthermore, we study the convergence rate of the sequence $\{x^k,y^k\}$ based on the 
Kurdyka-\L{}ojasiewicz(KL) property. Finally, we present the numerical results of Hankel matrix optimization
problem which has the form of \eqref{problem}.  

Throughout this paper, let $D_x$ and $D_y$ denote the diameters of $\mathrm{dom}$ $f$ 
and $\mathrm{dom}$ $g$ respectively, i.e.,
\begin{align} \label{Definition_of_diameter}
    D_x = \sup\limits_{x_1,x_2\in \mathrm{dom}f}\|x_1 -x_2\rVert,
    D_y = \sup\limits_{y_1,y_2\in \mathrm{dom}g}\|y_1 -y_2\rVert.
\end{align}

\begin{assumption} \label{Assumption 1}
    Assume that $f = f_1 +f_2$, 
    where $f_1$ is convex and smooth while $f_2$ is nonsmooth.
    Besides, assume that   
    $g = g_1 +g_2$, where $g_1$ is convex and smooth while $g_2$ is nonsmooth. 
\end{assumption}

\begin{assumption} \label{Assumption 2}
    Assume that $\mathrm{dom} f$ and $\mathrm{dom} g$ are bounded, i.e, $D_x<\infty$ and $D_y < \infty$. 
\end{assumption}

\section{Notation and preliminaries}
Throughout this paper, we use $\mathbb{N}$ to denote the set of nonnegative integers and $\mathcal{E}$, $\mathcal{E}_1$, $\mathcal{E}_2$ to denote the 
finite dimensional real Hilbert spaces. For two vectors $x$ and $y$ in $\mathbb{R}^n$, we use $\langle x,y\rangle$
to denote the inner product and $\|x\rVert$ to denote the Euclidean norm. Besides, for two matrices 
$X$ and $Y$ in $\mathbb{R}^{m\times n}$, the Frobenius norm is denoted by $\|X\rVert_F$ and 
the inner product is denoted by $\langle X,Y \rangle =\mathrm{tr}\left(X^TY\right)$.
Let $A: \mathcal{E}_1\rightarrow \mathcal{E}$ be a linear operator. We use $\lambda_{\max}(A^*A)$ to denote the maximum eignvalue value of $A^TA  $.

For a proper closed convex function $f$, we use $\partial f(x)$ to denote the subdifferential of $f$ at $x$, 
i.e.
\begin{align}
    \partial f(x) = \left\{\xi \in \mathbb{R}^n: f(y)- f(x) \geq\langle \xi, y-x \rangle \hspace{4pt} \forall y \in \mathbb{R}^n\right\}. 
    \nonumber
\end{align}
Besides, for a nonempty closed convex set $\mathcal{C}$, we use $\delta_{\mathcal{C}}$ to denote the indicator function, which
is defined as 
\begin{align}
    \delta_{\mathcal{C}}(x) = \left\{ 
        \begin{array}{cc}
            0  & x\in \mathcal{C},  \nonumber \\
            \infty & x \notin \mathcal{C}.  \nonumber
        \end{array}
     \right.  \nonumber
\end{align}
The normal cone of $\mathcal{C}$ at an $x\in\mathcal{C}$ is 
defined as $\mathcal{N}_{\mathcal{C}}(x):=\partial\delta_{\mathcal{C}}(x)$. 

In order to analyze the convergence in Section \ref{section_convergence_Lipschitz}, we also introduce the Abel's summation formula. Let
$\{a_t\}$ and $\{b_t\}$ be two sequences. Then, 
\begin{align}
    \sum\limits_{t=0}^k a_k(b_t - b_{t+1}) = (a_0b_0 - a_{k}b_{k+1})+ \sum\limits_{t=1}^{k} b_{t}(a_{t}-a_{t-1}). 
    \label{Abel formula}
\end{align}

Next, we introduce some important definitions that will be used in our convergence analysis. To start with, 
we consider the following contraint qualification: 
\begin{definition}
    {\rm{\textbf{(CQ).}}} Recall that $f_2$ and $g_2$ are defined in Assumption \ref{Assumption 1}. 
    We say the \textbf{CQ} holds if the following statement holds. 
    \begin{align}
        c \in A\hspace{2pt} \mathrm{ri}\left(\mathrm{dom}\hspace{2pt}f_2\right) +
    B\hspace{2pt} \mathrm{ri}\left(\mathrm{dom}\hspace{2pt}g_2\right).  \label{CQ} 
    \end{align}
\end{definition}
\begin{lemma}
    {\rm{\textbf{(Optimality condition).}}} \label{optimal}
    Suppose that the constraint qualification \eqref{CQ} holds. 
    We say that $x^*\in \mathrm{dom}f$ and $y^*\in \mathrm{dom}g$ are minimizers of problem $\eqref{problem}$ 
    if and only if there exists a vector $\bar{\lambda} \in \mathcal{E}$ such that the following statements hold.
    
    $(\romannumeral1)$ $0 \in \partial f(x^*) + A^*\bar{\lambda};  $

    $(\romannumeral2)$ $0 \in \partial g(y^*) + B^*\bar{\lambda};  $

    $(\romannumeral3)$ $Ax^* + By^* =c. $
    
\end{lemma}

Finally, we introduce the definitions and some useful lemmas of Kurdyka-\L{}ojasiewicz property and exponent.
\begin{definition}
    {\rm{\textbf{(KL property and exponent).}}} Suppose that $f$ is a proper closed convex function and it satisfies KL property 
    at an $\bar{x}\in \mathrm{dom} \partial f$ if there exist $r\in (0,\infty]$, a neighborhood $U$ of $\bar{x}$
    and a continuous concave function $\phi: [0,r) \rightarrow \mathbb{R}_+$ such that

    $(\romannumeral1)$ $\phi(0) = 0$, $\phi$ is differentiable on $(0,r)$ and $\phi' >0$.

    $(\romannumeral2)$ $\forall x\in U$ and $f(\bar{x}) \leq f(x) < f(\bar{x}) +r$, it holds that
                            \begin{align}
                                \phi'(f(x)-f(\bar{x}))\mathrm{dist}(0,\partial f(x)) \geq 1. \nonumber
                            \end{align}
    Furthermore, if $\phi(x) = \rho x^{1-\alpha}$ for some $\rho>0$ and $\alpha\in[0,1)$. Then we can 
    say that $f$ satisfies the KL property with exponent $\alpha$ at $\bar{x}$. 
\end{definition}

The next lemma taken from \cite[Lemma 3.10]{bolte2014proximal} plays an important role in analyzing the convergence rate of the whole sequence
$\{x^k,y^k\}$. 
\begin{lemma} \label{error_bound}
    Suppose that $f$ is a proper, closed, convex and level-bounded function. 
    Let $\Lambda := \Argmin f \neq \emptyset$. Suppose that $f$ satisfies the KL property with exponent $\alpha$ 
    at each point in $\Lambda$. Then there exist $\epsilon>0$, $r_0>0$ and $c_0>0$ such that
    \begin{align}
        \mathrm{dist}(x,\Lambda) \leq c_0 (f(x)-\inf f)^{1-\alpha} \nonumber
    \end{align}
    for any $x\in \partial f$ with $dist(x,\Lambda) \leq \epsilon$ and $\inf f <f(x) < \inf f+ r_0$.
\end{lemma}

\section{Algorithm framework }\label{section_convergence_Lipschitz} 
Before introducing our method for solving problem $\eqref{problem}$ under Assumption \ref{Assumption 1}, recall that $f=f_1 + f_2$, 
$g=g_1+g_2$ and  $f_1,g_1$ are smooth. 
Besides, our method for solving problem \eqref{problem} also relies on the following assumptions. 
\begin{assumption} \label{Assumption_Lipschitz_continuity}
    Suppose that $f_1$ is smooth and has Lipschitz continuous gradient with constant $L_f$. Similarly, 
    we can also suppose that $g_1$ is smooth and has Lipschitz continuous gradient with constant $L_g$.
\end{assumption}
Additionally,  we also consider when $f_1$ and $g_1$ have H\"older continous We say that $f_1$ and 
$g_1$ have H\"older continous gradients with exponents $\mu$, $\nu$ and constants $M_f$, $M_g$ respectively if and only if the following assumption hold.
\begin{assumption} \label{Assumption_holder}
   There exist $\mu \in (0,1)$, $\nu\in (0,1)$, $M_f>0$ and $M_g>0$,  it holds that 
    \begin{align}
        f_1(y) &\leq f_1(x) + \left\langle \nabla f_1(x), y-x \right\rangle +\frac{M_f}{\mu +1}
        \|y-x\rVert^{\mu +1},\quad \forall x,y\in \mathrm{dom}f \nonumber \\
        g_1(y) & \leq g_1(x) + \left\langle \nabla g_1(x), y-x \right\rangle +\frac{M_g}{\nu+1}
        \|y-x\rVert^{\nu+1}. \quad \forall x,y\in \mathrm{dom}g \label{Holder_property}
    \end{align}
\end{assumption}

Our algorithm is inspired by \cite{yu2020rc}. First, we use quadratic 
penalty function to modify  problem $\eqref{problem}$ which is shown in $\eqref{problem_2}$ 
with penalty parameter $\beta_k$:
\begin{align}
    \min f(x) + g(y) + \beta_k h(x,y),  
    \label{problem_2}
\end{align}
where $h(x,y) = \frac{1}{2}\|Ax+ By-c \rVert^2$.

Next, We use proximal gradient method to obtain $x^{k+1}$ while use Frank
Wolfe method to obtain $y^{k+1}$. Finally, we update $\beta_k$ by a certain rule. In Frank Wolfe step, we use the diminishing 
stepsize $\alpha_k = \frac{2}{k+2}$. Notice that the penalty function 
is quadratic. Therefore, to make sure that the algorithm is convergent, $\beta_k$ should be infinite 
as $k\rightarrow \infty$. Our algorithms are slight different considering the Assumption \ref{Assumption_Lipschitz_continuity} 
and the Assumption \ref{Assumption_holder}. 
If Assumption \ref{Assumption_Lipschitz_continuity} holds, we use the Lipschitz constant $L_f$ to get $x^{k+1}$ 
as shown in Algorithm \ref{alg1}. However, if Assumption \ref{Assumption_holder} holds, instead of using the 
Holder constant $M_f$, we use $H_k$ which is a function of $k$. That means in $k$-th step, we use $H_k$ for 
proximal point and then update $H_{k+1}$ under some rule for $k+1$ step. In practice, the $H_{k}$ is usually 
updated as follows. 
\begin{align}
    H_{k} = H_0(k+1)^{1-s\mu} \label{definition_Ht}, 
\end{align} 
where $s \in (-\infty,1)$ and $\mu$ is the H\"older exponent of $f_1$. 
More detailes can be seen in Algorithm \ref{alg2}.

\begin{algorithm}
	%\textsl{}\setstretch{1.8}
	\caption{Basic Algorithm for Lipschitz Continous Functions}
	\label{alg1}
	\begin{algorithmic}
		\STATE \textbf{Step 0} Choose $x^0 \in \mathrm{dom} f, y^0 \in \mathrm{dom} g$, $\beta_0>0$ and the positive sequences $\{\alpha_t\}$, $\{\beta_t\}$ 
        with $\lim\limits_{t\rightarrow\infty}\alpha_t=0$ and $\lim\limits_{t\rightarrow \infty}\beta_t=\infty $. 
        Let $\lambda_{A}=\lambda_{\max}(A^*A)$
		\STATE \textbf{Step 1} For $t=0,1,\cdots$ compute   
            \begin{align}
                &x^{t+1} \in \Argmin \langle \nabla f_1(x^t) + \beta_{t}A^*(Ax^t+By^t-c), x-x^t\rangle +\frac{L_f+\lambda_A \beta_t}{2}\|x-x^t\rVert^2 +f_2(x) \label{get_x_k} \\
                &u^t \in \Argmin \langle\nabla g_1(y^t) + \beta_{t}B^*(Ax^{t+1} +By^t-c),y\rangle +g_2(y) \label{get_uk}  \\
                &y^{t+1} = y^t +\alpha_t(u^t-y^t) \label{get_yk} 
            \end{align}
	\end{algorithmic}  
\end{algorithm}

\begin{algorithm}
	%\textsl{}\setstretch{1.8}
	\caption{ Algorithm for H\"older Continuous Functions}
	\label{alg2}
	\begin{algorithmic}
		\STATE \textbf{Step 0} Choose $x^0 \in \mathrm{dom}f, y^0 \in \mathrm{dom}g$, $s\in(-\infty,1)$, $r_0 >0$ and the sequences $\{\alpha_t\}$,
        $\{\beta_t\}$ with $\lim\limits_{t\rightarrow \infty}\alpha_t = 0$, $\lim\limits_{t\rightarrow\infty}\beta_t=\infty$. Compute $\lambda_A= \lambda_{\max}({A^*A})$.
		\STATE \textbf{Step 1} For $t=0,1,\cdots$ compute
            \begin{align}
                &x^{t+1} \in \Argmin \langle \nabla f_1(x^t) + \beta_{t}A^*(Ax^t+By^t-c), x-x^t\rangle +\frac{H_t+\lambda_{A} \beta_{t}}{2}\|x-x^t\rVert^2 +f_2(x) \label{get_xk+1_holder}  \\
                & u^t \in \Argmin \langle\nabla g_1(y^t) + \beta_{t}B^*(Ax^{t+1} +By^t-c),y\rangle +g_2(y) \label{get_uk_holder}  \\
                & y^{t+1} = y^{t} +\alpha_t(u^t-y^t)  \label{get_y_k_holder} \\
                & H_{t+1} = H_0(t+2)^{1-s\mu}
            \end{align}
	\end{algorithmic}  
\end{algorithm}

In order to establish the convergence results of the Algorithm \ref{alg1} and Algorithm  \ref{alg2}, we first present some auxiliary lemmas.
\begin{lemma}   \label{lemma_of_h}
    Let $\delta \in (0,1]$, $\beta_t = (t+1)^{\delta}$ and $\alpha_t=\frac{2}{t+2}$. Suppose that  
    $A: \mathcal{E}_1\rightarrow \mathcal{E}$, $B: \mathcal{E}_2\rightarrow \mathcal{E}$ are two linear maps and $x^*$, $y^*$ are two points satisfying 
    $Ax^*+By^*=c$. Define $h(x,y)=\frac{1}{2}\|Ax+By-c\rVert^2$. Then for any $x_1, x_2\in \mathcal{E}_1$, $y\in\mathcal{E}_2$, it holds that
    \begin{align}
        &(1-\alpha_t)(\beta_{t}-\beta_{t-1})h(x_1,y) + \alpha_t\beta_{t}h(x_1,y) + \alpha_t\beta_{t}\langle Ax_1+By-c,Ax^*-Ax_1\rangle \nonumber \\
        &+ \alpha_t\beta_{t}\langle Ax_2 +By -c, By^* -By\rangle \leq \alpha_t\beta_t \langle Ax_1 -Ax_2, By-By^* \rangle. \nonumber 
    \end{align} 
\end{lemma}
\begin{proof}
    First, notice that
    \begin{align}
        &(1-\alpha_t)(\beta_{t}-\beta_{t-1}) -\alpha_t \beta_{t} = \frac{t}{t+2}\left((t+1)^{\delta}
        -t^{\delta}\right) -\frac{2}{t+2}(t+1)^{\delta} \nonumber \\
        & \overset{\mathop{(a)}}{\leq} \frac{t}{t+2}\delta t^{\delta -1} -\frac{2}{t+2}(t+1)^{\delta} 
        \overset{\mathop{(b)}}{\leq} \frac{t^{\delta}}{t+2} -\frac{2(t+1)^{\delta}}{t+2} \leq -\frac{(t+1)^{\delta}}{t+2} \leq 0, 
        \nonumber
    \end{align}
    where $(a)$ holds because the function $x \mapsto x^{\delta}$ is concave and $(b)$ holds
    because $\delta \leq 1$.
    Therefore, we have that $(1-\alpha_t)(\beta_{t}-\beta_{t-1})\leq \alpha_t \beta_{t}$. 
    Hence, 
    \begin{align}
        &(1-\alpha_t)(\beta_{t}-\beta_{t-1})h(x_1,y) + \alpha_t\beta_{t}h(x_1,y) + \alpha_t\beta_{t}\langle Ax_1+By-c,Ax^*-Ax_1\rangle \nonumber \\
        &+ \alpha_t\beta_{t}\langle Ax_2 +By -c, By^* -By\rangle  \nonumber \\
        \overset{\mathop{(a)}}{\leq}& 2\alpha_t\beta_{t} h(x_1,y) + \alpha_t\beta_{t}\langle Ax_1+By-c,Ax^*-Ax_1\rangle+ \alpha_t\beta_{t}\langle Ax_2 +By -c, By^* -By\rangle \nonumber \\
        \overset{\mathop{(b)}}{=}& \alpha_t\beta_t\|Ax_1+By-(Ax^*+ By^*)\rVert^2 +\alpha_t\beta_{t} \langle Ax_1 +By-(Ax^*+By^*),A(x^*-x_1) \rangle \nonumber\\
        &+ \alpha_t\beta_{t}\langle Ax_2 +By-(Ax^*+By^*), B(y^*-y) \rangle \nonumber \\
        =& {\alpha_t\beta_{t}}\|A(x_1-x^*)\rVert^2 + {\alpha_t\beta_{t}}\|B(y-y^*)\rVert^2 +2\alpha_t\beta_{t}\langle Ax_1-Ax^*, By-By^*\rangle -\alpha_t\beta_{t}\|A(x_1-x^*)\rVert^2       \nonumber \\
        & -\alpha_t\beta_{t}\|B(y-y^*)\rVert^2 +\alpha_t\beta_{t}\langle B(y-y^*), A(x^*-x_1) \rangle +\alpha_t\beta_{t}\langle A(x_2-x^*), B(y^*-y) \rangle  \nonumber \\
        =&\alpha_t\beta_t \langle Ax_1 -Ax_2, By-By^* \rangle, \nonumber
    \end{align} 
    where $(a)$ holds because $(1-\alpha_t)(\beta_{t}-\beta_{t-1})\leq \alpha_t \beta_{t}$, $(b)$ holds because $Ax^*+ By^* =c$. 
    Therefore, the desired result follows. 
\end{proof}
\begin{lemma} \label{lemma_of_sum_Axk-Axk+1}
    Let  $\{x^t\}$, $\{y^t\}$ be generated in Algorithm \ref{alg1} or Algorithm \ref{alg2}, $y^*$ be a point 
    in $\mathrm{dom}$g, $\alpha_t = \frac{2}{t+2}$ and $\beta_{t} =(t+1)^{\delta}$ for some $\delta \in (0,1]$. 
    Furthermore, we suppsoe that $\{x^t\}$ and $\{y^t\}$ are bounded. Let $D_2 =\max\limits_t \left| \langle Ax^t,By^t \rangle \right|$. 
    Then we have for each $k\in\mathbb{N}$ that  
    \begin{align}
        \left| \sum\limits_{t = 0}^k\alpha_t\beta_{t}(t+1)(t+2)\left\langle Ax^t- Ax^{t+1},By^t-By^* \right\rangle \right| 
        \leq 4\beta_0D_2 +\frac{16+8\delta}{1+\delta}D_2(t+1)^{1+\delta} \nonumber
    \end{align}
\end{lemma}
\begin{proof}
    First, we have that 
    \begin{align}
        &\left| \sum\limits_{t = 0}^k\alpha_t\beta_{t}(t+1)(t+2)\left\langle Ax^t- Ax^{t+1},By^t-y^* \right\rangle \right| \nonumber \\
        &\leq \left| \sum\limits_{t = 0}^k\alpha_t\beta_{t}(t+1)(t+2)\left\langle Ax^t- Ax^{t+1},By^* \right\rangle \right|
        + \left| \sum\limits_{t = 0}^k\alpha_t\beta_{t}(t+1)(t+2)\left\langle Ax^t- Ax^{t+1},By^t \right\rangle \right|
        \label{sum_Axk-Axk+1_two_parts}
    \end{align}
    Next, in order to further estimate the first term in \eqref{sum_Axk-Axk+1_two_parts}, 
    we use the Abel's summation formula by letting $a_t = \alpha_k\beta_t(t+1)(t+2)$ and $b_t = \langle Ax^t,By^* \rangle$ in \eqref{Abel formula}.
    \begin{align} 
        &\left| \sum\limits_{t = 0}^k\alpha_t\beta_{t}(t+1)(t+2)\left\langle Ax^t- Ax^{t+1},By^* \right\rangle \right| \nonumber \\
        \overset{\mathop{(a)}}{=}& \Bigg| 2\alpha_0\beta_0 \left\langle Ax^0, By^* \right\rangle -\alpha_k\beta_{k}(k+1)(k+2)\left\langle  Ax^{k+1},By^* \right\rangle \nonumber \\
        &+ \left. \sum\limits_{t=1}^{k-1} \left[ -\alpha_t\beta_{t}(t+1)(t+2)\left\langle Ax^t, By^* \right\rangle
        +  \alpha_{t+1}\beta_{t+1}(t+2)(t+3)\left\langle Ax^t, By^* \right\rangle\right] \right|    \nonumber \\
        \leq &\left| 2\alpha_0\beta_0 \left\langle Ax^0, By^* \right\rangle \right| +\left|\alpha_k\beta_{k}(k+1)(k+2)\left\langle  Ax^{k+1},By^* \right\rangle\right| \nonumber \\
        &+\left| \sum\limits_{t=1}^{k-1} \left(\alpha_{t+1}\beta_{t+1}(t+2)(t+3) -\alpha_t\beta_{t}(t+1)(t+2)\right)\left\langle Ax^t, By^* \right\rangle\right| \nonumber \\
        \overset{\mathop{(b)}}{\leq} &2\left| \beta_0 \left\langle Ax^0, By^* \right\rangle \right|
        + 2\left|(k+1)^{1+\delta}\left\langle  Ax^{k+1},By^* \right\rangle\right|
        +2\left| \sum\limits_{t=1}^{k-1} \left((t+2)^{1+\delta} -(t+1)^{1+\delta}\right)\left\langle Ax^t, By^* \right\rangle\right| 
        \nonumber \\ 
        \overset{\mathop{(c)}}{\leq} & 2 \beta_0 \left|\left\langle Ax^0, By^* \right\rangle \right|
        + 2(k+1)^{1+\delta}\left|\left\langle  Ax^{k+1},By^* \right\rangle\right|
        +2(1+\delta) \sum\limits_{t=1}^{k-1} (t+2)^{\delta} \left|\left\langle Ax^t, By^* \right\rangle\right|,  
        \label{Axk_Axk+1,By*}
    \end{align}
    where $(a)$ holds due to Abel's summation formula, $(b)$ holds due to the definitions 
    of $\alpha_t$, $\beta_{t}$ and $\alpha_t\beta_t(t+1)(t+2)=2(t+1)^{1+\delta}$ and $(c)$ holds because
    the function $x \mapsto x^{1+\delta}$ is convex.
    
    To further analyze the second term in \eqref{sum_Axk-Axk+1_two_parts}, we use the method which is 
    similar to Abel' summation formula. 
    \begin{align}
        &\left| \sum\limits_{t = 0}^k\alpha_t\beta_{t}(t+1)(t+2)\left\langle Ax^t- Ax^{t+1},By^t \right\rangle \right| \nonumber \\
        \overset{(a)}{=}& \Bigg| 2\alpha_0\beta_0 \langle Ax^0, By^0 \rangle -\alpha_k\beta_k(k+1)(k+2)\langle Ax^{t+1}, By^t \rangle  \nonumber \\
        &+\left.\sum\limits_{t=0}^k\left(-\alpha_t\beta_t(t+1)(t+2)\langle Ax^t, By^t \rangle+\alpha_{t+1}\beta_{t+1}(t+2)(t+3)
        \langle Ax^{t}, By^{t+1} \rangle\right) \right| \nonumber \\
        =& \Bigg| 2\alpha_0\beta_0 \langle Ax^0, By^0 \rangle -\alpha_k\beta_k(k+1)(k+2)\langle Ax^{t+1}, By^t \rangle  \nonumber \\
        &+\sum\limits_{t=0}^k\left(-\alpha_t\beta_t(t+1)(t+2)\langle Ax^t, By^t \rangle+\alpha_t\beta_{t}(t+1)(t+2) 
        \langle Ax^{t}, By^{t+1} \rangle\right)  \nonumber \\
        & \left.+ \sum\limits_{t=0}^k\left(\alpha_{t+1}\beta_{t+1}(t+2)(t+3) - \alpha_t\beta_t(t+1)(t+2)\langle Ax^t, By^{t+1} \rangle\right) \right| \nonumber \\
       \leq & \left| 2\beta_0 \left\langle Ax^0, By^0 \right\rangle \right|
        + \left|2(k+1)^{1+\delta}\left\langle  Ax^{k+1},By^k \right\rangle\right| +\left| \sum\limits_{t=1}^{k-1}\alpha_t \beta_{t}(t+1)(t+2) \left\langle Ax^t, By^{t+1}-By^t \right\rangle \right| \nonumber \\
        & + \left|\sum\limits_{t=1}^{k-1}\left(\alpha_{t+1}\beta_{t+1}(t+2)(t+3) - \alpha_t\beta_{t}(t+1)(t+2)\right)\left\langle Ax^t, By^{t+1} \right\rangle \right| \nonumber \\
        \overset{\mathop{(b)}}{\leq} & 2 \beta_0 \left| \left\langle Ax^0, By^0 \right\rangle \right|
        + 2(k+1)^{1+\delta} \left| \left\langle  Ax^{k+1},By^k \right\rangle\right| 
        + \sum\limits_{t=1}^{k-1} \left| \alpha_t \beta_{t} (t+1)(t+2) \left\langle Ax^t, \alpha_tB(u^t-y^t) \right\rangle \right| \nonumber \\
        & +\left|\sum\limits_{t=1}^{k-1}\left(\alpha_{t+1}\beta_{t+1}(t+2)(t+3) - \alpha_t\beta_{t}(t+1)(t+2)\right)\left\langle Ax^t, By^{t+1} \right\rangle \right|
        \nonumber \\
        \overset{\mathop{(c)}}{\leq} & 2 \beta_0 \left| \left\langle Ax^0, By^0 \right\rangle \right|
        + 2(k+1)^{1+\delta} \left| \left\langle  Ax^{k+1},By^k \right\rangle\right| 
        + \sum\limits_{t=1}^{k-1} \left| \alpha_t^2 \beta_{t} (t+1)(t+2) \left\langle Ax^t, B(u^t-y^t) \right\rangle \right| \nonumber \\
        & +2(1+\delta) \sum\limits_{t=1}^{k-1} (t+2)^{\delta} \left| \left\langle Ax^t, By^{t+1} \right\rangle\right|
        \nonumber \\
        \overset{\mathop{(d)}}{\leq} &2 \beta_0 \left|\left\langle Ax^0, By^0 \right\rangle \right|
        + 2(k+1)^{1+\delta}\left|\left\langle  Ax^{k+1},By^k \right\rangle\right| 
        + 4 \sum\limits_{t=1}^{k-1} \left|\beta_{t}\left\langle Ax^t, Bu^t-By^t \right\rangle \right| \nonumber \\
        & +2(1+\delta) \sum\limits_{t=1}^{k-1} (t+2)^{\delta} \left| \left\langle Ax^t, By^{t+1} \right\rangle\right|, 
        \label{Axk_Axk+1,Byk}
    \end{align}
    where $(a)$ holds by rearranging the terms which is similar to Abel's formula, $(b)$ holds due to \eqref{get_yk} and \eqref{get_y_k_holder}, $(c)$ holds 
    because the function $x \mapsto x^{1+\delta}$ is convex and $\alpha_t\beta_t(t+1)(t+2) = 2(t+1)^{1+\delta}$. 
    Besides, $(d)$ holds because of the definition of $\beta_t$ and $\alpha_t^2(t+1)(t+2)\leq 4$. 
    Combining  $\eqref{Axk_Axk+1,By*}$ with $\eqref{Axk_Axk+1,Byk}$, we obtain that 
    \begin{align}
        &\left| \sum\limits_{t = 0}^k\alpha_t\beta_{t}(t+1)(t+2)\left\langle Ax^t- Ax^{t+1},By^t-By^* \right\rangle \right| \nonumber \\
        \leq& 2 \beta_0 \left|\left\langle Ax^0, By^* \right\rangle \right|
        + 2(k+1)^{1+\delta} \left|\left\langle  Ax^{k+1},By^* \right\rangle\right|
        +2 \beta_0 \left| \left\langle Ax^0, By^0 \right\rangle \right|
        + 2(k+1)^{1+\delta} \left| \left\langle  Ax^{k+1},By^k \right\rangle\right| \nonumber \\
        &+ 2(1+\delta) \sum\limits_{t=1}^{k-1} (t+2)^{\delta} \left|\left\langle Ax^t, By^* \right\rangle\right|
        + 2(1+\delta) \sum\limits_{t=1}^{k-1} (t+2)^{\delta} \left| \left\langle Ax^t, By^{t+1} \right\rangle\right| \nonumber \\
        &+ 4 \sum\limits_{t=1}^{k-1}  (t+1)^{\delta} \left| \left\langle Ax^t, Bu^t-By^t \right\rangle \right| \nonumber \\
        \leq& 2 \beta_0 \left|\left\langle Ax^0, By^* \right\rangle \right|
        + 2(k+1)^{1+\delta} \left|\left\langle  Ax^{k+1},By^* \right\rangle\right|
        +2 \beta_0 \left| \left\langle Ax^0, By^0 \right\rangle \right|
        + 2(k+1)^{1+\delta} \left| \left\langle  Ax^{k+1},By^k \right\rangle\right| \nonumber \\
        &+ 2(1+\delta) \sum\limits_{t=1}^{k-1} (t+2)^{\delta} \left|\left\langle Ax^t, By^* \right\rangle\right|
        + 2(1+\delta) \sum\limits_{t=1}^{k-1} (t+2)^{\delta} \left| \left\langle Ax^t, By^{t+1} \right\rangle\right| \nonumber \\
        &+ 4 \sum\limits_{t=1}^{k-1}  (t+1)^{\delta} \left| \left\langle Ax^t, Bu^t \right\rangle \right| 
        + 4 \sum\limits_{t=1}^{k-1} (t+1)^{\delta} \left| \left\langle Ax^t, Bu^t \right\rangle \right| \nonumber  \\
        \overset{\mathop{(a)}}{\leq}& 4\beta_0D_2 + 4(k+1)^{1+\delta}D_2 +4(1+\delta)D_2\sum\limits_{t=1}^{k-1} (t+2)^{\delta} 
        + 8D_2\sum\limits_{t=1}^{k-1} (t+1)^{\delta} \nonumber \\
        \overset{\mathop{(b)}}{\leq} &  4\beta_0D_2 + 4(k+1)^{1+\delta}D_2 +4(1+\delta)D_2\int_0^{k+1} x^{\delta} dx 
        + 8D_2 \int_0^{k} x^{\delta} dx         \nonumber \\
        \leq & 4\beta_0D_2 + 4(k+1)^{1+\delta}D_2 +4(k+1)^{1+\delta}D_2   
        + \frac{8}{1+\delta}D_2 k^{1+\delta} \nonumber \\
        =& 4\beta_0D_2 + 8(k+1)^{1+\delta}D_2 + \frac{8}{1+\delta}D_2 k^{1+\delta} \leq 4\beta_0D_2 + 8(k+1)^{1+\delta}D_2 + \frac{8}{1+\delta}D_2 (k+1)^{1+\delta} \nonumber \\
        =& 4\beta_0D_2 +\frac{16+8\delta}{1+\delta}D_2(k+1)^{1+\delta}, \nonumber
    \end{align}
    where $(a)$ holds because of the definition of $D_2$ and $(b)$ holds because of the function $x\mapsto x^{\delta}$ is 
    increasing. Therefore, the desired result follokws. 
\end{proof}

\section{Convergence analysis}
In this secction, we first establish the convergence results of Algorithm \ref{alg1} and Algorithm \ref{alg2}. Next, 
we further study the convergence rate of the sequence $\{x^k,y^k\}$ based on the KL property and exponents. 

\subsection{Convergence rate analysis}
In order to get the convergence results of Algorithm \ref{alg1} and Algorithm \ref{alg2}, We first introduce some important lemmas for our analysis. 

\begin{lemma} \label{lemma_Ax+By-c}
    Consider \eqref{problem}. Suppose that the Assumption \ref{Assumption 1}, Assumption \ref{Assumption 2} 
    hold and  $x^*$, $y^*$ are the solutions to problem \eqref{problem}. Besides, suppose that 
    CQ \eqref{CQ} holds and $\bar{\lambda}$ are defined in Lemma \ref{CQ}. Additionally,  
    there exists a sequence $\{\tau_k\}$ such that $f(x^k)+ g(y^k)+\frac{\beta_k}{2}\|Ax^k+By^k-c\rVert^2 -f(x^*)-g(y^*)\leq \tau_k$. 
    Then, it holds that.  
    \begin{align}
        \|Ax^k+By^k-c\rVert \leq \frac{\|\bar{\lambda}\rVert}{\beta_k} + \sqrt{\frac{\|\bar{\lambda}\rVert^2}{\beta_k^2}+\frac{2\tau_k}{\beta_k}}
        \nonumber
    \end{align}
    and
    \begin{align}
        \left|f(x^k)+ g(y^k) -f(x^*)-g(y^*) \right| \leq & 
        \max\left\{ \tau_k, \|\bar{\lambda}\rVert\|Ax^k+ By^k-c\rVert \right\} \nonumber \\
        \leq & \max\left\{ \tau_k, \frac{\|\bar{\lambda}\rVert^2}{\beta_k} + \|\bar{\lambda}\rVert\sqrt{\frac{\|\bar{\lambda}\rVert^2}{\beta_k^2}+\frac{2\tau_k}{\beta_k}} \right\}. 
        \nonumber
    \end{align} 
\end{lemma}
\begin{proof}
    First, due to Lemma \ref{CQ}, there exists $\xi_1 \in \partial f$, $\xi_2\in \partial g$ such that 
    $0 \in \xi_1 + A^*\bar{\lambda}$ and $0\in \xi_2+B^*\bar{\lambda}$.
    Therefore, we obtain that 
    \begin{align}
        0&= \langle \xi_1+A^*\bar{\lambda}, x^k -x^*\rangle+ \langle \xi_2+B^*\bar{\lambda}, y^k-y^*\rangle \nonumber \\
        & =\langle \xi_1, x^k -x^* \rangle +\langle \xi_2, y^k -y^*\rangle +\langle \bar{\lambda}, A(x^k-x^*) \rangle +\langle \bar{\lambda}, B(y^k-y^*) \rangle \nonumber \\
        &\overset{\mathop{(a)}}{\leq} f(x^k) -f(x^*) + g(y^k) -g(y^*) + \langle \bar{\lambda}, Ax^k+By^k-c\rangle, \nonumber
    \end{align}
    where $(a)$ holds because $f$ is convex 
    and $Ax^* + By^* = c$. 
    
    Second, notice that 
    \begin{align}
       -\|\bar{\lambda}\rVert \cdot\|Ax^k+By^k-c\rVert\leq -\langle \bar{\lambda}, Ax^k+By^k-c\rangle \leq f(x^k) -f(x^*) + g(y^k) -g(y^*). \label{F-F^*}
    \end{align}
    Recall that $f(x^k)+ g(y^k)+\frac{\beta_k}{2}\|Ax^k+By^k-c\rVert^2\leq \tau_k$. Then we obtain that 
    \begin{align}
        0&\leq f(x^k) -f(x^*) + g(y^k) -g(y^*) + \|\bar{\lambda}\rVert \cdot \|Ax^k+By^k-c\rVert \nonumber \\
        & \leq -\frac{\beta_k}{2}\|Ax^k+By^k-c\|^2 + \tau_k +\|\bar{\lambda}\rVert\|Ax^k+By^k-c\|. \nonumber
    \end{align}
    Therefore, solving this inequality, we have that 
    \begin{align}
        \|Ax^k+By^k-c\rVert \leq \frac{\|\bar{\lambda}\rVert+\sqrt{\|\bar{\lambda}\rVert^2+2\beta_k\tau_k}}{\beta_k}
        =\frac{\|\bar{\lambda\rVert}}{\beta_k}+\sqrt{\frac{\|\bar{\lambda}\rVert^2}{\beta_k^2}+\frac{2\tau_k}{\beta_k}}. \label{Axk-Byk-c}
    \end{align}

    Finally, based on \eqref{F-F^*} and \eqref{Axk-Byk-c}, we obtain that 
    \begin{align}
        f(x^k) + g(y^k) -f(x^*)- g(y^*) \geq -\frac{\|\bar{\lambda}\rVert^2}{\beta_k}- \|\bar{\lambda}\rVert\sqrt{\frac{\|\bar{\lambda}\rVert^2}{\beta_k^2}+\frac{2\tau_k}{\beta_k}}. 
        \label{f+g-f*-g*_geq}
    \end{align}
    Besides, recall that $f(x^k)+ g(y^k)+\frac{\beta_k}{2}\|Ax^k+By^k-c\rVert^2 -f(x^*)-g(y^*)\leq \tau_k$. 
    We obtain that 
    \begin{align}
        f(x^k)+ g(y^k) -f(x^*)-g(y^*)\leq -\frac{\beta_k}{2}\|Ax^k+By^k-c\rVert^2+ \tau_k \leq \tau_k.   
        \label{f+g-f*-g*_leq}
    \end{align}
    Combining \eqref{f+g-f*-g*_geq} with \eqref{f+g-f*-g*_leq}, the desired result follows. 
\end{proof}


\begin{theorem} \label{theorem_f+beta_h}
    Consider \eqref{problem} and the Algorithm \ref{alg1}, Algorithm \ref{alg2} for solving problem \eqref{problem}. 
    Suppose that the Assumptions \ref{Assumption 1}, \ref{Assumption 2} 
    and CQ \eqref{CQ} hold. Let $x^*$ and $y^*$ are the solutions to problem \eqref{problem}, 
    $\alpha_t=\frac{2}{t+2}$ and $\beta_{t} =(t+1)^{\delta}$ for all $t\in \mathbb{N}$. 
    Then we have that the following statements hold:

    (\romannumeral1) Suppose that the Assumption \ref{Assumption_holder} holds and the sequences 
    $\{x^t\}$, $\{y^t\}$ and $\{u^t\}$ are generated by Algorithm \ref{alg2}. Recall that 
    $H_t = H_0(t+1)^{1-s\mu}$. Then, for any $k\geq t$ and $k\in \mathbb{N}$, 
    \begin{align}
        &f(x^{k}) + g(y^{k}) +\frac{\beta_{k-1}}{2}\|Ax^k+By^k-c\rVert^2 - f(x^*) -g(y^*)  \nonumber \\
        \leq& \frac{\omega_1}{k(k+1)}+ \frac{\omega_2}{(k+1)^{1-\delta}}+ \frac{\omega_3}{(k+1)^{\nu}} 
        + \frac{\omega_4}{(k+1)^{\frac{1-s\mu}{1-\mu}-1}}+ \frac{2\omega_5}{k+1}+ \frac{\omega_5}{k^{\frac{(1-s\mu)(1+\mu)}{1-\mu}-1}}, 
        \nonumber
    \end{align}
    where $\omega_1 = 4\beta_0D_2$, $\omega_2 = \frac{(32+16\delta)D_2}{1+\delta}+ 2(\lambda_AD_x^2+ \lambda_BD_y^2)$, 
    $\omega_3 = \frac{2^{\nu+1}}{\nu+1}M_gD_y^{\nu+1}$, $\omega_4 = 2D_x^2H_0$, $\omega_5 = \omega_0D_x^2$  
    and $\omega_0 = \left(\frac{2M_f}{(1+\mu)H_0}\right)^{\frac{1}{1-\mu}} $. Here, $M_f$ and $\mu$ are the 
    H\"older constant and exponent of $f_1$, $M_g$ and $\nu$ are the H\"older constant and exponent of $g_1$. 
    $D_x$ and $D_y$ are given in \eqref{Definition_of_diameter},    
    $\lambda_{A} =\lambda_{\max}(A^*A)$, $\lambda_{B} = \lambda_{\max}(B^*B)$, $D_2 = \max\limits_{x,y}\left| \langle Ax,By\rangle \right|$.
    Thanks to Assumption \ref{Assumption 2}, we have that $D_2 < \infty$. 

    Furthermore, we have that the convergence rate of function value is 
    \begin{align} \label{convergence_rate_function}
        \left|f(x^k)+ g(y^k) -f(x^*)-g(y^*) \right|
        \leq & \max\left\{ \tau_k, \frac{\|\bar{\lambda}\rVert^2}{(k+1)^{\delta}} + \|\bar{\lambda}\rVert\sqrt{\frac{\|\bar{\lambda}\rVert^2}{(k+1)^{2\delta}}+\frac{2\tau_k}{(k+1)^{\delta}}} \right\}
    \end{align}
    and the convergence rate of feasiblity is  
    \begin{align} \label{convergence_rate_feasiblity}
        \|Ax^k+By^k-c\rVert \leq \frac{\|\bar{\lambda}\rVert}{(k+1)^{\delta}} + \sqrt{\frac{\|\bar{\lambda}\rVert^2}{(k+1)^{2\delta}}+\frac{2\tau_k}{(k+1)^{\delta}}}, 
    \end{align}
    where $\tau_k = \frac{\omega_1}{k(k+1)}+ \frac{\omega_2}{(k+1)^{1-\delta}}+ \frac{\omega_3}{(k+1)^{\nu}} 
        + \frac{\omega_4}{(k+1)^{\frac{1-s\mu}{1-\mu}-1}}+ \frac{2\omega_5}{k+1}+ \frac{\omega_5}{k^{\frac{(1-s\mu)(1+\mu)}{1-\mu}-1}}$. 
    
    (\romannumeral2) Suppose that the Assumption \ref{Assumption_Lipschitz_continuity} holds and the sequences 
    $\{x^k\}$, $\{y^k\}$ and $\{u^k\}$ are generated by Algorithm \ref{alg1}. Then, 
    \begin{align}
        f(x^{k}) + g(y^{k}) +\frac{\beta_{k-1}}{2}\|Ax^k+By^k-c\rVert^2 - f(x^*) -g(y^*)  
        \leq \frac{2\omega_2}{(k+1)^{1-\delta}} +\frac{c_1}{k+1} +\frac{\omega_1}{k(k+1)}, \nonumber
    \end{align}
    where $c_1 = 2L_fD_x^2+ 2L_gD_y^2$. Here, $L_f$, $L_g$ are the Lipschitz constants of $f_1$, $g_1$ respectively. 
    
    Similarly, we have that the convergence rate of function value 
    and the convergence rate of feasiblity are the same to  \eqref{convergence_rate_function} and \eqref{convergence_rate_feasiblity}. 
    However, in this case, $\tau_k$ is different from $(\romannumeral1)$, which is shown as follows. 
    \begin{align}
        \tau_k = \frac{2\omega_2}{(k+1)^{1-\delta}} +\frac{c_1}{k+1} +\frac{\omega_1}{k(k+1)} \nonumber
    \end{align}
\end{theorem}
\begin{proof}
    Recall that $\lambda_{A} = \lambda_{\max}(A^*A)$, $h(x,y) =\frac{1}{2}\|Ax+By-c\rVert^2$.  
    Let $(x^t,y^t)$ be the $t$-th iteration result. Due to the convexity of $\mathrm{dom}f$, 
    we have that the point $x^t + \alpha_t(x^*-x^t)$ is also in $\mathrm{dom}f$.   
    Then we  analyze the property of \eqref{get_xk+1_holder} in Algorithm \ref{alg2}. 
    First, notice that 
    \begin{align}
        &f(x^{t+1})+\beta_th(x^{t+1},y^t) +\langle \nabla f_1(x^t), x^{t+1}-x^t\rangle +\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2 \nonumber \\
        \overset{\mathop{(a)}}{\leq} & f_1(x^{t+1})+f_2(x^{t+1}) +\langle \nabla f_1(x^t), x^{t+1}-x^t\rangle +\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2 
         \nonumber \\
        &+\beta_t\langle A^*(Ax^t+By^t-c), x^{t+1}-x^t\rangle +\beta_th(x^t,y^t) +\frac{\beta_t\lambda_A}{2}\| x^{t+1}-x^t\rVert^2 \nonumber\\
        \overset{\mathop{(b)}}{\leq}& f_1(x^{t+1})+ f_2(x^t+\alpha_t(x^*-x^t)) +\alpha_t\langle \nabla f_1(x^t), x^*-x^t\rangle  \nonumber \\
        &+\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle + \beta_th(x^t,y^t) + 
        \frac{H_t+\beta_t\lambda_A}{2}\alpha_t^2\|x^*-x^t\rVert^2 \nonumber \\
        \overset{\mathop{(c)}}{\leq} & f_1(x^{t+1})+ (1-\alpha_t)f_2(x^t)+\alpha_tf_2(x^*) +\alpha_t\langle \nabla f_1(x^t), x^*-x^t\rangle  \nonumber \\
        &+\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle + \beta_th(x^t,y^t) + 
        \frac{H_t+\beta_t\lambda_A}{2}\alpha_t^2\|x^*-x^t\rVert^2, \nonumber
    \end{align}
    where $(a)$ holds because the function $\beta_th(\cdot,y)$ has Lipschitz continuous gradient with constant 
    $\lambda_A\beta_t$, $(b)$ holds because of \eqref{get_x_k} 
    and the point $x^t+\alpha_t(x^*-x^t)$ being in the $\mathrm{dom} f$ and $(c)$ holds because $f_2$ is convex.
    
    By rearranging the terms and defining $\zeta_{t+1} = f_1(x^{t+1})-f_1(x^k)-\langle \nabla f_1(x^t),x^{t+1}-x^t\rangle$, we obtain 
    that 
    \begin{align}
        &f(x^{t+1}) +\beta_th(x^{t+1},y^t) \nonumber \\
        \leq& f_1(x^t)+\zeta_{t+1}+(1-\alpha_t)f_2(x^t)+\alpha_tf_2(x^*)+\alpha_t\langle \nabla f_1(x^t), x^*-x^t\rangle +\frac{H_t+\beta_t\lambda_A}{2}\alpha_t^2\|x^*-x^t\rVert^2 \nonumber \\
        &+\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle + \beta_th(x^t,y^t) -\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2 \nonumber \\
        \overset{\mathop{(a)}}{\leq} & (1-\alpha_t)f_1(x^t)+(1-\alpha_t)f_2(x^t) +\alpha_t f_1(x^*)+\alpha_tf_2(x^*)+\frac{H_t+\beta_t\lambda_A}{2}\alpha_t^2\|x^*-x^t\rVert^2 \nonumber \\
        &+\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle + \beta_th(x^t,y^t) -\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2+\zeta_{t+1} \nonumber \\
        \leq & (1-\alpha_t)f_1(x^t)+(1-\alpha_t)f_2(x^t) +\alpha_t f_1(x^*)+\alpha_tf_2(x^*)+\frac{H_t+\beta_t\lambda_A}{2}\alpha_t^2\|x^*-x^t\rVert^2 \nonumber \\
        &+\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle + \beta_th(x^t,y^t) -\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2 +\frac{M_f}{\mu+1}\|x^{t+1}-x^t\rVert^{\mu+1},  \label{f_beta_h}
    \end{align}
    where $(a)$ holds because $f_1$ is convex and the last inequality holds because $f_1$ has H\"older continuous gradient with constant $M_f$ and 
    exponent $\mu$. 

    Next, we consider the term $-\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2 +\frac{M_f}{\mu+1}\|x^{t+1}-x^t\rVert^{\mu+1}$ in 
    the above inequality. 
    Define 
    \begin{align}
        (x)^+ = \left\{ 
        \begin{array}{cc}
            0  & x<0,  \nonumber \\
            x & x \geq 0.  \nonumber
        \end{array}
     \right.  \nonumber
    \end{align}
    Notice that 
    \begin{align}
        &-\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2 +\frac{M_f}{\mu+1}\|x^{t+1}-x^t\rVert^{\mu+1} \leq \left(-\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2 +\frac{M_f}{\mu+1}\|x^{t+1}-x^t\rVert^{\mu+1}\right)^+.
        \label{f+beta_h_1}
    \end{align}
    Without loss of generality, suppose that  $\left(-\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2 +\frac{M_f}{\mu+1}\|x^{t+1}-x^t\rVert^{\mu+1}\right)^+ \geq 0$, 
    which implies
    \begin{align}
        \|x^{t+1}-x^t\rVert \leq \left(\frac{2M_f}{(1+\mu)H_0}\right)^{\frac{1}{1-\mu}}\frac{1}{(t+1)^{\frac{1-s\mu}{1-\mu}}}. 
    \end{align}
    Therefore, by letting $\omega_0 = \left(\frac{2M_f}{(1+\mu)H_0}\right)^{\frac{1+\mu}{1-\mu}}$, we obtain that 
    \begin{align}
        \left(-\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2 +\frac{M_f}{\mu+1}\|x^{t+1}-x^t\rVert^{\mu+1}\right)^+ =& -\frac{H_t}{2}\|x^{t+1}-x^t\rVert^2 +\frac{M_f}{\mu+1}\|x^{t+1}-x^t\rVert^{\mu+1} \nonumber \\
        \leq& \frac{M_f}{\mu+1}\|x^{t+1}-x^t\rVert^{\mu+1} \leq \omega_0 \frac{1}{(t+1)^{\frac{(1-s\mu)(1+\mu)}{1-\mu}}}. 
        \label{f+beta_h_2}
    \end{align}
    Combining \eqref{f_beta_h} with \eqref{f+beta_h_1} and \eqref{f+beta_h_2}, we obtian that 
    \begin{align} 
        &f(x^{t+1}) +\beta_th(x^{t+1},y^t) \nonumber \\
        \leq & (1-\alpha_t)f_1(x^t)+(1-\alpha_t)f_2(x^t) +\alpha_t f_1(x^*)+\alpha_tf_2(x^*)+\frac{H_t+\beta_t\lambda_A}{2}\alpha_t^2\|x^*-x^t\rVert^2 \nonumber \\
        &+\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle + \beta_th(x^t,y^t) +\omega_0(t+1)^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}}.  \label{f+beta_h_holder}
    \end{align}  
    
    By rearranging the terms in \eqref{f+beta_h_holder}, we have that 
    \begin{align} 
        &f(x^{t+1}) +\beta_th(x^{t+1},y^t) -f(x^*)\nonumber \\
        \leq & (1-\alpha_t)(f(x^t)-f(x^*))+\frac{H_t+\beta_t\lambda_A}{2}\alpha_t^2\|x^*-x^t\rVert^2  \nonumber \\
        &+\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle+ \beta_th(x^t,y^t) +\omega_0(t+1)^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}}.  \label{f+beta_h-f*}
    \end{align}
    Next, we obtain that 
    \begin{align}
        &g(y^{t+1}) + \beta_th(x^{t+1},y^{t+1}) \nonumber \\
        \overset{\mathop{(a)}}{\leq}& g_1(y^{t})+ \langle \nabla g_1(y^t)+\beta_tB^*(Ax^{t+1}+By^t-c), y^{t+1}-y^t\rangle + 
        \frac{M_g}{\nu+1}\|y^{t+1}-y^t\rVert^{\nu+1} \nonumber\\
        &+g_2(y^{t+1}) +\beta_th(x^{t+1},y^t)+\frac{\beta_t\lambda_B}{2}\|y^{t+1}-y^t\rVert^2 \nonumber \\
        \overset{\mathop{(b)}}{\leq}& g_1(y^{t})+ \alpha_t\langle \nabla g_1(y^t)+\beta_tB^*(Ax^{t+1}+By^t-c), u^t-y^t\rangle + 
        \frac{M_g}{\nu+1}\alpha_t^{\nu+1}\|u^t-y^t\rVert^{\nu+1} \nonumber\\
        &+\alpha_tg_2(u^t)+(1-\alpha_t)g_2(y^{t}) +\beta_th(x^{t+1},y^t)+\frac{\beta_t\lambda_B}{2}\alpha_t^2\|u^t-y^t\rVert^2\nonumber \\
        \overset{\mathop{(c)}}{\leq}& g_1(y^{t})+ \alpha_t\langle \nabla g_1(y^t)+\beta_tB^*(Ax^{t+1}+By^t-c), y^*-y^t\rangle + 
        \frac{M_g}{\nu+1}\alpha_t^{\nu+1}\|u^t-y^t\rVert^{\nu+1} \nonumber\\
        &+\alpha_tg_2(y^*)+(1-\alpha_t)g_2(y^{t}) +\beta_th(x^{t+1},y^t)+\frac{\beta_t\lambda_B}{2}\alpha_t^2\|u^t-y^t\rVert^2 \nonumber \\
        \overset{\mathop{(d)}}{\leq}& g_1(y^{t})+\alpha_tg_1(y^*)-\alpha_tg_1(y^t)+ \alpha_t\beta_t\langle B^*(Ax^{t+1}+By^t-c), y^*-y^t\rangle + 
        \frac{M_g}{\nu+1}\alpha_t^{\nu+1}\|u^t-y^t\rVert^{\nu+1} \nonumber\\
        &+\frac{\beta_t\lambda_B}{2}\alpha_t^2\|u^t-y^t\rVert^2+\alpha_tg_2(y^*)+(1-\alpha_t)g_2(y^{t}) +\beta_th(x^{t+1},y^t) \nonumber \\
        \overset{\mathop{(e)}}{\leq}& (1-\alpha_t)g(y^{t})+\alpha_tg(y^*)+ \alpha_t\beta_t\langle B^*(Ax^{t+1}+By^t-c), y^*-y^t\rangle  \nonumber\\
        &+\beta_th(x^{t+1},y^t)+ 
        \frac{M_g}{\nu+1}\alpha_t^{\nu+1}D_y^{\nu+1}+\frac{\beta_t\lambda_B}{2}\alpha_t^2D_y^2 ,  \nonumber
    \end{align}
    where $(a)$ holds because $g_1$ has H\"older continuous gradient with exponent $\nu$ and constant $M_g$
    and the function $h(x,y)$ has Lipschitz continuous gradient with constant $\lambda_B$ with respect to $y$, 
    $(b)$ holds because $g_2$ is convex as well as the definition of $y^{t+1}$ in \eqref{get_yk} or \eqref{get_y_k_holder}, $(c)$ holds because of \eqref{get_x_k} or \eqref{get_xk+1_holder}, 
    $(d)$ holds because $g_1$ is convex and $(e)$ holds because of definition of $D_y$.  
    By rearranging terms, we have that
    \begin{align}
        g(y^{t+1}) + \beta_th(x^{t+1},y^{t+1}) - g(y^*)
        \leq& (1-\alpha_t)(g(y^t)-g(y^*))+ \alpha_t\beta_t\langle B^*(Ax^{t+1}+By^t-c), y^*-y^t\rangle  \nonumber \\
        & + \beta_th(x^{t+1},y^t)+ 
        \frac{M_g}{\nu+1}\alpha_t^{\nu+1}D_y^{\nu+1}+\frac{\beta_t\lambda_B}{2}\alpha_t^2D_y^2.   \label{g+beta_h-g*} 
    \end{align}
    Combining \eqref{f+beta_h-f*} with \eqref{g+beta_h-g*}, we have that 
    \begin{align}
        &f(x^{t+1})+ g(y^{t+1})+ \beta_{t}h(x^{t+1},y^{t+1})-f(x^*)-g(y^*)  \nonumber \\
        \leq& (1-\alpha_t)(f(x^t)+g(y^t)+\beta_{t-1}h(x^t,y^t)-f(x^*)-g(y^*)) + (1-\alpha_t)(\beta_t-\beta_{t-1})h(x^t,y^t) +\alpha_t\beta_th(x^t,y^t) \nonumber \\
        &+ \frac{H_t+\beta_t\lambda_A}{2}\alpha_t^2
        \|x^t-x^*\rVert^2+\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle +\omega_0D_x^2(t+1)^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}} \nonumber \\
        &+ \alpha_t\beta_t\langle B^*(Ax^{t+1}+By^t-c), y^*-y^t\rangle + 
        \frac{M_g}{\nu+1}\alpha_t^{\nu+1}D_y^{\nu+1}+\frac{\beta_t\lambda_B}{2}\alpha_t^2D_y^2. 
    \end{align}
    Let $x^t = x_1$, $x^{t+1} = x_2$ and $y^t= y$ in Lemma \ref{lemma_of_h}, we have that  
    \begin{align}
        &f(x^{t+1})+ g(y^{t+1})+ \beta_{t}h(x^{t+1},y^{t+1})-f(x^*)-g(y^*)  \nonumber \\
        \leq& (1-\alpha_t)(f(x^t)+g(y^t)+\beta_{t-1}h(x^t,y^t)-f(x^*)-g(y^*)) + \frac{H_t+\beta_t\lambda_A}{2}\alpha_t^2
        \|x^t-x^*\rVert^2   \nonumber \\
        &+\omega_0D_x^2(t+1)^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}} +\frac{M_g}{\nu+1}\alpha_t^{\nu+1}D_y^{\nu+1}+\frac{\beta_t\lambda_B}{2}\alpha_t^2D_y^2 
        +\alpha_t\beta_t\langle Ax^t-Ax^{t+1}, By^t-By^* \rangle. 
    \end{align}

    Next, multiplying $(t+1)(t+2)$ to the both sides of the above inequality and letting 
    $\Gamma_t = t(t+1)(f(x^t)+g(y^t)+\beta_{t-1}h(x^t,y^t)-f(x^*)-g(y^*))$, we obtain that  
    \begin{align}
        \Gamma_{t+1} \leq& \Gamma_t+(t+1)(t+2)\alpha_t\beta_t\langle Ax^t-Ax^{t+1}, By^t-By^*\rangle +\omega_0D_x^2(t+1)(t+2) (t+1)^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}} \nonumber \\
        &+ (t+1)(t+2)\frac{M_g}{\nu+1}\alpha_t^{\nu+1}D_y^{\nu+1}+(t+1)(t+2)\frac{\beta_t\lambda_B}{2}\alpha_t^2D_y^2 + (t+1)(t+2)\frac{H_t+\beta_t\lambda_A}{2}\alpha_t^2D_x^2\nonumber \\
        =&\Gamma_t+(t+1)(t+2)\alpha_t\beta_t\langle Ax^t-Ax^{t+1}, By^t-By^*\rangle +\omega_0D_x^2(t+1)(t+2) (t+1)^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}} \nonumber \\
        &+\frac{2^{\nu+1}}{\nu+1}\frac{t+1}{(t+2)^{\nu}}M_gD_y^{\nu+1} +\frac{2(t+1)}{t+2}\beta_t\lambda_BD_y^2 +2(H_t+\beta_t\lambda_A)\frac{t+1}{t+2}D_x^2 \nonumber \\
        \leq &\Gamma_t+(t+1)(t+2)\alpha_t\beta_t\langle Ax^t-Ax^{t+1}, By^t-By^*\rangle +\omega_0D_x^2(t+1)(t+2) (t+1)^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}} \nonumber \\
        &+\frac{2^{\nu+1}}{\nu+1}(t+1)^{1-\nu}M_gD_y^{\nu+1} +2\beta_t\lambda_BD_y^2 +2(H_t+\beta_t\lambda_A)D_x^2 \nonumber
    \end{align}
    Summing both sides of the above inequality from $t=0$ to $k-1$ and noticing that $\Gamma_0 =0$, we have that
    \begin{align}
        \Gamma_k \leq& \sum\limits_{t=0}^{k-1} \Big[(t+1)(t+2)\alpha_t\beta_t\langle Ax^t-Ax^{t+1}, By^t-By^*\rangle +(t+1)(t+2) (t+1)^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}}\omega_0D_x^2  \nonumber \\
        &\left.+\frac{2^{\nu+1}}{\nu+1}(t+1)^{1-\nu}M_gD_y^{\nu+1} +2\beta_t\lambda_BD_y^2 +2(H_t+\beta_t\lambda_A)D_x^2  \right] \nonumber  \\
        \overset{\mathop{(a)}}{\leq} & \left(4\beta_0+\frac{16+8\delta}{1+\delta}(k+1)^{1+\delta}\right)D_2 +\sum\limits_{t=0}^{k-1}(t+1)(t+2)(t+1)^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}}\omega_0D_x^2 \nonumber \\
        &+\sum\limits_{t=0}^{k-1}\frac{2^{\nu+1}}{\nu+1}(t+1)^{1-\nu}M_gD_y^{\nu+1} +2(\lambda_AD_x^2+ \lambda_BD_y^2)\sum\limits_{t=1}^{k-1} \beta_t
        +2D_x^2\sum\limits_{t=0}^{k-1}H_t              \nonumber \\
        \overset{\mathop{(b)}}{\leq} & \left(4\beta_0+\frac{16+8\delta}{1+\delta}(k+1)^{1+\delta}\right)D_2+\omega_0D_x^2 \max\left\{ 2k, k^2(k+1)k^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}} \right\} \nonumber \\
        & + \frac{2^{\nu+1}}{\nu+1}M_gD_y^{\nu+1}k^{2-\nu} +2(\lambda_AD_x^2+ \lambda_BD_y^2)k\beta_{k-1} +2D_x^2kH_{k-1}, \nonumber 
    \end{align}
    where $(a)$ holds because of Lemma \ref{lemma_of_sum_Axk-Axk+1}, $(b)$ holds because 
    $(t+1)^{1-\nu} \leq k^{1-\nu}$, $\beta_t \leq \beta_{k-1}$, $H_t\leq H_{k-1}$ for all $0\leq t\leq k-1$ 
    and the sequence $\{(t+1)(t+2)(t+1)^{-\frac{(1-s\mu)(1+\mu)}{1-\mu}}\}$ is also monotone. 
    Recall that $\Gamma_k = k(k+1)(f(x^k)+g(y^k)+\beta_{k-1}h(x^k,y^k)-f(x^*)-g(y^*))$. Then we have that
    \begin{align}
        &f(x^k)+g(y^k)+\beta_{k-1}h(x^k,y^k)-f(x^*)-g(y^*) \nonumber \\
        \leq& \frac{4\beta_0D_2}{k(k+1)}+\frac{(16+8\delta)D_2}{1+\delta}\frac{(k+1)^{\delta}}{k}
        +\frac{2^{\nu+1}}{\nu+1}M_gD_y^{\nu+1}\frac{k^{1-\nu}}{k+1}+2(\lambda_BD_y^2+\lambda_AD_x^2)\frac{k^{\delta}}{k+1} 
        +2D_x^2H_0\frac{k^{\frac{1-s\mu}{1-\mu}}}{k+1} \nonumber \\
        &+ \omega_0D_x^2\max\left\{\frac{2}{k+1}, k^{1-\frac{(1-s\mu)(1+\mu)}{1-\mu}}\right\} \nonumber \\
        \leq & \frac{4\beta_0D_2}{k(k+1)}+\frac{(16+8\delta)D_2}{1+\delta}\frac{(k+1)^{\delta}}{k+1}\frac{k+1}{k}
        +\frac{2^{\nu+1}}{\nu+1}M_gD_y^{\nu+1}\frac{(k+1)^{1-\nu}}{k+1}+2(\lambda_BD_y^2+\lambda_AD_x^2)\frac{(k+1)^{\delta}}{k+1} 
         \nonumber \\
        &+2D_x^2H_0\frac{(k+1)^{\frac{1-s\mu}{1-\mu}}}{k+1}+ \omega_0D_x^2\frac{2}{k+1}+\omega_0D_x^2k^{1-\frac{(1-s\mu)(1+\mu)}{1-\mu}}  \nonumber \\
        \leq & \frac{4\beta_0D_2}{k(k+1)}+\left(\frac{(32+16\delta)D_2}{1+\delta}+2(\lambda_BD_y^2+\lambda_AD_x^2)\right)\frac{1}{(k+1)^{1-\delta}}
        +\frac{2^{\nu+1}}{\nu+1}M_gD_y^{\nu+1}\frac{1}{(k+1)^{\nu}}     \nonumber \\
        &+2D_x^2H_0\frac{1}{(k+1)^{\frac{1-s\mu}{1-\mu}-1}}+ \omega_0D_x^2\frac{2}{k+1}+\omega_0D_x^2\frac{1}{k^{\frac{(1-s\mu)(1+\mu)}{1-\mu}-1}}.  \nonumber
    \end{align}
    The convergence rates of function value and feasiblity follows from Lemma \ref{lemma_Ax+By-c}. 

    $(\romannumeral2)$ 
    Notice that  $f_1$ has Lipschitz continuous, then we use the constant $L_f$ instead of $H_t$ in proximal 
    gradient step. Therefore,  we have that $H_t = M_f =L_f$ and $\mu=1$ in \eqref{f_beta_h}. Then we have that  
    \begin{align}
        &f(x^{t+1}) +\beta_th(x^{t+1},y^t) \nonumber \\
        \leq & (1-\alpha_t)f_1(x^t)+(1-\alpha_t)f_2(x^t) +\alpha_t f_1(x^*)+\alpha_tf_2(x^*)+\frac{L_f+\beta_t\lambda_A}{2}\alpha_t^2\|x^*-x^t\rVert^2 \nonumber \\
        &+\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle + \beta_th(x^t,y^t) -\frac{L_f}{2}\|x^{t+1}-x^t\rVert^2 +\frac{L_f}{2}\|x^{t+1}-x^t\rVert^{2}  \nonumber \\
        \leq & (1-\alpha_t)f_1(x^t)+(1-\alpha_t)f_2(x^t) +\alpha_t f_1(x^*)+\alpha_tf_2(x^*)+\frac{L_f+\beta_t\lambda_A}{2}\alpha_t^2\|x^*-x^t\rVert^2 \nonumber \\
        &+\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle + \beta_th(x^t,y^t). \nonumber 
    \end{align}
    By rearranging terms and invoking $f=f_1+f_2$, we have that 
    \begin{align}
        &f(x^{t+1}) +\beta_th(x^{t+1},y^t) -f(x^*) \nonumber \\
        \leq & (1-\alpha_t)(f(x^t)-f(x^*)) +\frac{L_f+\beta_t\lambda_A}{2}\alpha_t^2\|x^*-x^t\rVert^2 
        +\alpha_t\beta_t\langle A^*(Ax^t+By^t-c), x^*-x^t\rangle + \beta_th(x^t,y^t). \label{f+beta_h-f*_Lipschitz}
    \end{align}
    Next, if $g_2$ is Lipschitz continuous with constant $L_g$, we have that $M_g = L_g$ and $\nu=1$ in 
    \eqref{g+beta_h-g*}. Then we obtain that 
    \begin{align}\label{g+beta_h-g*_Lipschitz}
        g(y^{t+1}) + \beta_th(x^{t+1},y^{t+1}) - g(y^*) 
        \leq& (1-\alpha_t)(g(y^t)-g(y^*))+ \alpha_t\beta_t\langle B^*(Ax^{t+1}+By^t-c), y^*-y^t\rangle   \nonumber \\
        &+ \beta_th(x^{t+1},y^t)+ 
        \frac{L_g+\beta_t\lambda_B}{2}\alpha_t^{2}D_y^{2}.      
    \end{align}      
    Summing \eqref{f+beta_h-f*_Lipschitz} and \eqref{g+beta_h-g*_Lipschitz}, we have that 
    \begin{align}
        &f(x^{t+1}) +g(y^{t+1}) +\beta_{t}h(x^{t+1},y^{t+1}) -f(x^*) -g(y^*) \nonumber \\
        \leq& (1-\alpha_t)\left(f(x^t) +g(y^t) -f(x^*)-g(y^*)\right) +\beta_{t}h(x^t,y^t)+\alpha_t\beta_{t}\langle Ax^t+By^t-c,Ax^*-Ax^t\rangle \nonumber \\
        &+ \alpha_t\beta_{t}\langle Ax^{t+1} +By^t -c, By^* -By^t\rangle +\frac{L_f+\lambda_{A} \beta_{t}}{2}\alpha_t^2D_x^2+ \frac{L_g +\lambda_{B} \beta_{t}}{2}\alpha_t^2D_y^2 \nonumber \\
        =& (1-\alpha_t)\left(f(x^t) +g(y^t)+\beta_{t-1}h(x^t,y^t) -f(x^*)-g(y^*)\right)+\alpha_t\beta_{t}\langle Ax^t+By^t-c,Ax^*-Ax^t\rangle \nonumber \\
        &+ \alpha_t\beta_{t}\langle Ax^{t+1} +By^t -c, By^* -By^t\rangle +\frac{L_f+\lambda_{A} \beta_{t}}{2}\alpha_t^2D_x^2+ \frac{L_g +\lambda_{B} \beta_{t}}{2}\alpha_t^2D_y^2
        \nonumber \\
        & +(1-\alpha_t)(\beta_{t}-\beta_{t-1})h(x^t,y^t) + \alpha_t\beta_{t} h(x^t,y^t)  \nonumber \\
        \leq & (1-\alpha_t)\left(f(x^t) +g(y^t)+\beta_{t-1}h(x^t,y^t) -f(x^*)-g(y^*)\right) 
        +\alpha_t\beta_t\langle Ax^t-Ax^{t+1}, By-By^*\rangle \nonumber \\
        &+  \frac{L_f+\lambda_{A} \beta_{t}}{2}\alpha_t^2D_x^2+ \frac{L_g +\lambda_{B} \beta_{t}}{2}\alpha_t^2D_y^2,  \label{F_k-F_k+1_Lipschitz}
    \end{align}
    where the last inequality holds because of Lemma \ref{lemma_of_h}. 

    Let $\Gamma_t =t(t+1) \left(f(x^t) +g(y^t)+\beta_{t-1}h(x^t,y^t) -f(x^*)-g(y^*)\right)$. 
    Then, multiplying $(t+1)(t+2)$ to the both sides of the above inequality and invoking the fact that 
    $(t+1)(t+2)\alpha_t^2 = (t+1)(t+2)\frac{4}{(t+2)^2} \leq 4$, we have that 
    \begin{align}
        \Gamma_{t+1} \leq& \Gamma_t + \frac{L_f+\lambda_{A} \beta_{t}}{2}(t+1)(t+2)\alpha_t^2D_x^2+ \frac{L_g +\lambda_{B} \beta_{t}}{2}(t+1)(t+2)\alpha_t^2D_y^2 \nonumber \\
        & +\alpha_t\beta_{t} (t+1)(t+2)\langle Ax^t -Ax^{t+1}, By^t-By^{*} \rangle  \nonumber \\
        \leq& \Gamma_t + 2(L_f+\lambda_{A} \beta_{t})D_x^2+ 2(L_g +\lambda_{B} \beta_{t})D_y^2 +\alpha_t\beta_{t}(t+1)(t+2)\langle Ax^t- Ax^{t+1}, By^t-By^{*}\rangle \nonumber \\
        =& \Gamma_t + (2\lambda_{A} D_x^2+ 2\lambda_{B} D_y^2)\beta_{t} +2L_fD_x^2+2L_gD_y^2 + \alpha_t\beta_{t}(t+1)(t+2)\langle Ax^t- Ax^{t+1}, By^t-By^{*}\rangle. 
    \end{align}
    Summing both sides of the above inequality from $t=0$ to $k-1$, we have that
    \begin{align}
        &\Gamma_k \leq \sum_{t = 0}^{k-1}\left[ (2\lambda_{A} D_x^2+ 2\lambda_{B} D_y^2) \beta_{t} + (2L_fD_x^2+2L_gD_y^2 ) \right] +\sum_{t=0}^{k-1}\alpha_t \beta_{t}(t+1)(t+2)\langle Ax^t-Ax^{t+1}, By^t -By^*\rangle\nonumber \\
        & \overset{\mathop{(a)}}{\leq} k\beta_{k-1}(2\lambda_{A} D_x^2+ 2\lambda_{B} D_y^2)+ k(2L_fD_x^2+2L_gD_y^2 ) +\sum_{t=0}^{k-1}\alpha_t \beta_{t}(t+1)(t+2)\langle Ax^t-Ax^{t+1}, By^t -By^*\rangle \nonumber \\
        & \overset{\mathop{(b)}}{\leq}k\beta_{k-1}(2\lambda_{A} D_x^2+ 2\lambda_{B} D_y^2)+ k(2L_fD_x^2+2L_gD_y^2 ) 
        +4\beta_0D_2 + \frac{16+8\delta}{1+\delta}D_2(k+1)^{1+\delta}, \label{iterate Gammak}
    \end{align}
    where $(a)$ holds because $\beta_t \leq \beta_{k-1}$ for all $0 \leq t\leq k-1$ and $(b)$ holds because of 
    Lemma \ref{lemma_of_sum_Axk-Axk+1}. 
    Recall that $\omega_1 = 4\beta_0D_2$, $\omega_2= 2\lambda_{A} D_x^2+ 2\lambda_{B} D_y^2+\frac{16+8\delta}{1+\delta}D_2$, $c_1=2L_fD_x^2+2L_gD_y^2$, we have that
    \begin{align}
        \Gamma_k \leq& k\beta_{k-1}(2\lambda_{A} D_x^2+ 2\lambda_{B} D_y^2)+ k(2L_fD_x^2+2L_gD_y^2 ) 
        +4\beta_0D_2 + \frac{16+8\delta}{1+\delta}D_2(k+1)^{1+\delta} \nonumber \\
        \leq& (k+1)^{1+\delta}(2\lambda_{A} D_x^2+ 2\lambda_{B} D_y^2)+ k(2L_fD_x^2+2L_gD_y^2 ) 
        +4\beta_0D_2 + \frac{16+8\delta}{1+\delta}D_2(k+1)^{1+\delta} \nonumber \\ 
        \leq& \omega_2k^{1+\delta} + c_1k + \omega_1.     
    \end{align} 
    Because $\Gamma_k = k(k+1)\left(f(x^k)+ g(y^k)+ \beta_{k-1}h(x^k,y^k) -f(x^*)-g(y^*)\right)$, we have that
    \begin{align}
        &f(x^k) +g(y^k)+\beta_{k-1}h(x^k,y^k) -f(x^*)-g(y^*) \leq \frac{(k+1)^{\delta}\omega_2}{k} + \frac{c_1}{k+1}+\frac{\omega_1}{k(k+1)}  \nonumber\\
        & \leq \frac{k+1}{k}\frac{\omega_2}{(k+1)^{1-\delta}} +\frac{c_1}{k+1} +\frac{\omega_1}{k(k+1)} 
        \leq \frac{2\omega_2}{(k+1)^{1-\delta}} +\frac{c_1}{k+1} +\frac{\omega_1}{k(k+1)}. \label{Fk-F*_leq}
    \end{align}
    The convergence rates of function value and feasiblity follows from Lemma \ref{lemma_Ax+By-c}. 
\end{proof}

\begin{remark}
    Now, we discuss how to choose the parameters $\delta$ and $s$ to get the best convergence rates of 
function values and feasiblity. 

$(\romannumeral1)$ Consider the Algorithm \ref{alg2}. 
Recall the Theorem \ref{theorem_f+beta_h} and we have that 
\begin{align}
    &\tau_k = \frac{\omega_1}{k(k+1)}+ \frac{\omega_2}{(k+1)^{1-\delta}}+ \frac{\omega_3}{(k+1)^{\nu}} 
    + \frac{\omega_4}{(k+1)^{\mu_0-1}}+ \frac{2\omega_5}{k+1}+ \frac{\omega_5}{k^{\mu_0(1+\mu)-1}}, \nonumber \\
    &\|Ax^k+By^k-c\rVert \leq \frac{\|\bar{\lambda\rVert}}{(k+1)^{\delta}}+\sqrt{\frac{\|\bar{\lambda}\rVert^2}{(k+1)^{2\delta}}+\frac{2\tau_k}{(k+1)^{\delta}}}. \nonumber
\end{align}
where $\mu_0 = \frac{1-s\mu}{1-\mu}$. 

Since $0<s<1$, we obtain that $\mu_0 -1 =\frac{1-s\mu}{1-\mu}-1 >0$ and $\mu_0(1+\mu)-1>\mu_0-1>0$. 
By the definition of $\tau_k$, it holds that 
\begin{align}
    \tau_k \leq \max\left\{ \mathcal{O}\left(\frac{1}{(k+1)^{1-\delta}}\right), 
    \mathcal{O}\left(\frac{1}{(k+1)^{\nu}}\right),\mathcal{O}\left(\frac{1}{(k+1)^{\mu_0-1}}\right) \right\}. \label{convergence_tau_k}
\end{align}
Therefore, we have that
\begin{align}
    \|Ax^k+ By^k -c\rVert \leq \max\left\{\mathcal{O}\left( \frac{1}{(k+1)^{\delta}} \right), 
    \mathcal{O}\left( \frac{1}{\sqrt{k+1}}\right), \mathcal{O}\left( \frac{1}{(k+1)^{\frac{\delta+\nu}{2}}}\right), 
    \mathcal{O}\left( \frac{1}{(k+1)^{\frac{\delta+\mu_0-1}{2}}}\right)\right\}. \label{convergence_Ax+By-c}
\end{align} 
By Theorem \ref{theorem_f+beta_h}, we obtain that 
\begin{align}
    \left|f(x^k)+g(y^k) -f(x^*) -g(y^*)\right| \leq \max\left\{ \tau_k, \|\bar{\lambda}\rVert\|Ax^k+By^k-c\rVert \right\}. \nonumber
\end{align}
From \eqref{convergence_tau_k} and \eqref{convergence_Ax+By-c}, we konw that the fastest convergence rate 
of function value is $\max\left\{ \mathcal{O}\left( \frac{1}{(k+1)^{\nu}} \right), \left( \frac{1}{\sqrt{k+1}} \right) \right\}$. 
Therefore, we first consider the case when $\nu \geq \frac{1}{2}$. 
In this case,  we choose
\begin{align}
    \delta = \frac{1}{2}, \hspace{4pt}  s\leq \frac{3}{2} -\frac{1}{2\mu}. 
\end{align}
such that $\frac{\delta+\nu}{2} \geq \frac{1}{2}$, $\mu_0-1=\frac{1-s\mu}{1-\mu}-1\geq \frac{1}{2}$ and 
$\frac{\delta+\mu_0-1}{2}\geq \frac{1}{2}$. 
Hence, we obtain that 
\begin{equation}
    \begin{array}{c}
        \tau_k \leq  \mathcal{O}\left( \frac{1}{(k+1)^{\frac{1}{2}}} \right), \nonumber \\
        \|Ax^k+ By^k -c\rVert \leq \mathcal{O} \left( \frac{1}{(k+1)^{\frac{1}{2}}} \right). \nonumber
    \end{array}
\end{equation}
and 
\begin{align}
    f(x^k)+g(y^k) -f(x^*) -g(y^*) \leq \max\left\{ \tau_k, \|\bar{\lambda}\rVert\|Ax^k+By^k-c\rVert \right\} 
    \leq \mathcal{O} \left( \frac{1}{(k+1)^{\frac{1}{2}}} \right). \nonumber 
\end{align}

Else, if $\nu<\frac{1}{2}$, wew choose 
\begin{align}
    \nu \leq \delta \leq 1-\nu, \hspace{4pt} s\leq\frac{1-(1+\nu)(1-\mu)}{\mu} \nonumber 
\end{align}
such that $\mu_0-1=\frac{1-s\mu}{1-\mu}-1 \geq \nu$, $\frac{\delta+\nu}{2}\geq \frac{1}{2}$ and $
\frac{\delta+\mu_0-1}{2} \geq \frac{1}{2}$. Therefore,
\begin{equation}
    \begin{array}{c}
        \tau_k \leq  \mathcal{O}\left( \frac{1}{(k+1)^{\nu}} \right), \nonumber \\
        \|Ax^k+ By^k -c\rVert \leq \mathcal{O} \left( \frac{1}{(k+1)^{\nu}} \right), \nonumber
    \end{array}
\end{equation}
and 
\begin{align}
    f(x^k)+g(y^k) -f(x^*) -g(y^*) \leq \max\left\{ \tau_k, \|\bar{\lambda}\rVert\|Ax^k+By^k-c\rVert \right\} 
    \leq \mathcal{O} \left( \frac{1}{(k+1)^{\nu}} \right). \nonumber 
\end{align} 

$(\romannumeral2)$ Consider the Algorithm \ref{alg1}. 
Recall that $\tau_k= \frac{2\omega_2}{(k+1)^{1-\delta}}+ \frac{c_1}{k+1}+ \frac{\omega_1}{k(k+1)}$. 
Hence, we have that
\begin{align}
    \tau_k\leq \mathcal{O}\left( \frac{1}{(k+1)^{1-\delta}} \right). \nonumber
\end{align} 
Due to Theorem \ref{theorem_f+beta_h}, we have that 
$\|Ax^k+ By^k- c\rVert \leq \max\left\{ \mathcal{O} \left( \frac{1}{(k+1)^{\delta}} \right), 
\mathcal{O} \left( \frac{1}{\sqrt{k+1}} \right)\right\}$. 
Therefore, we choose $\delta = \frac{1}{2}$, then 
\begin{equation}
    \begin{array}{c}
        \left| f(x^k)+ g(y^k)- f(x^*)- g(y^*) \right| \leq \mathcal{O}\left( \frac{1}{\sqrt{k+1}} \right), \nonumber \\
        \|Ax^k+ By^k- \rVert \leq \mathcal{O}\left( \frac{1}{\sqrt{k+1}} \right).   \nonumber
    \end{array}
\end{equation}

\end{remark}



\subsection{KL property and convergence rate to the solution set}
Now, we have established the convergence rates of the function value $f(x^k)+ g(y^k) -f(x^*)- g(y^*)$ and the 
feasible condition $\|Ax^k + By^k - c\rVert$. However, the sequence $\{x^k,y^k\}$ may not be 
convergent. In order to further study the convergence rate to the solution set of the sequence $\{x^k,y^k\}$, we will use the KL property and
exponent. 

First, we assume that the functions $x \mapsto f(x) - \bar{\lambda}^TAx$ and $y\mapsto g(y) - \bar{\lambda}^TBy$ 
satisfy the KL property with exponent $\alpha$. Then we have that the function $(x,y) \mapsto f(x)+ g(y) -\bar{\lambda}^T(Ax+By-c)$ 
also has the KL exponent $\alpha$ (see Theorem 3.3 in \cite{li2018calculus}). 

Next, we introduce a useful lemma which indicate the relationship of KL exponent between the original function 
and the corresponding Lagrangian function. 
\begin{lemma} \label{lemma_of_Lagrangian_KL_exponent}
    Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a proper, closed and convex function. $A\in \mathbb{R}^{m\times n}$ 
    and $b\in \mathbb{R}^m$ satisfy $b \in A \mathrm{ri} \mathrm{dom} f$. Let $F(x) = f(x) + \delta_{\{b\}}(Ax)$.  
    Let $\bar{\lambda}$ be a Lagrangian multiplier for the following problem
    \begin{align}
        \min\hspace{4pt} &f(x) \nonumber \\
        s.t. \hspace{4pt} & Ax = b \nonumber 
    \end{align}  
    Suppose that $\emptyset \neq \Argmin F$ and $F_{\bar{\lambda}}(x) := f(x) -\bar{\lambda}^TAx$ satisfies the KL property with exponent $\alpha$ 
    at an $\bar{x}\in \Argmin F$. Furthermore, if $\emptyset \neq \mathrm{ri} (\Argmin F_{\bar{\lambda}}) \cap \{x: Ax = b\}$ or the 
    $\Argmin F$ is a polyhedron, then 
    $F$ also satisfies KL property at $\bar{x}$ with exponent $\alpha$. 
\end{lemma}
\begin{proof}
    First, because $\bar{\lambda}$ is a Lagrangian multiplier, we have that
    \begin{align}
        F(\bar{x}) =\inf F(x) = f(\bar{x}) = \inf F_{\bar{\lambda}}(x) \leq F_{\bar{\lambda}}(\bar{x}) 
        \leq F(\bar{x})   \nonumber
    \end{align}
    and 
    \begin{align}
        \bar{x} \in \Argmin F =\Argmin F_{\bar{\lambda}} \cap \{x: Ax=b\}   \nonumber
    \end{align}
    where the last equality holds because of \cite[Theorem 28.1]{Rockafellar+1970}.
    Second, since $F_{\bar{\lambda}}$ satisfies KL property with exponent $\alpha$ at $\bar{x}$, we have that there 
    exists $\epsilon>0$,  $r_0>0$ and $\bar{c}>0$ such that for any $x$ satisfying $\|x - \bar{x}\rVert \leq \epsilon$ and 
    $F_{\bar{\lambda}}(\bar{x})<F_{\bar{\lambda}}(x)< F_{\bar{\lambda}}(\bar{x})+ r_0$, it holds that 
    \begin{align}
        \mathrm{dist}(x,\Argmin F_{\bar{\lambda}}) \leq \bar{c}(F_{\bar{\lambda}}(x) - F_{\bar{\lambda}}(\bar{x}))^{1-\alpha} \nonumber
    \end{align}
    Finally, for any $x$ satisfying $\|x - \bar{x}\rVert \leq \epsilon$ and 
    $F(\bar{x})<F(x)< F(\bar{x})+ r_0$,  by using these two results, we obtain that there exists a constant $\kappa$ such that 
    \begin{align}
        &\mathrm{dist} (x, \Argmin F)  = \mathrm{dist} \left(x,\Argmin F_{\bar{\lambda}} \cap \{x: Ax=b\}\right) 
        \overset{\mathop{(a)}}{\leq} \kappa \mathrm{dist}(x, \Argmin F_{\bar{\lambda}}) \nonumber \\
        & \leq \kappa \bar{c} \mathrm{dist}(F_{\bar{\lambda}}(x) - F_{\bar{\lambda}}(\bar{x}))^{1-\alpha} = 
        \kappa \bar{c} \mathrm{dist}(F(x) - F(\bar{x}))^{1-\alpha}
    \end{align}
    where $(a)$ holds because of \cite[Corollary 3]{bauschke1999strong}.
    Hence, the desired result follows. 
\end{proof}

Now, we can deduce the convergence rate to the solution set of the sequences generated by Algorithm \ref{alg1}. 
\begin{theorem} \label{Theorem_of_KL}
    Consider the problem \eqref{problem}. Suppose that $f(x) = f_0(x) + \delta_{\Xi}(x)$ and $g(x) = g_0(x) + \delta_{\Delta}(x)$ where 
    $\Xi$, $\Delta$ are two compact and convex sets and $f_0$ and $g_0$ are two real-valued functions.     
    Let $F(x,y)= f(x)+ g(y)+ \delta_{\mathcal{D}}(x,y)$, where $\mathcal{D} = \{(x,y)| Ax+By=c\}$. Suppose that the function $F_{\bar{\lambda}} = f(x) + g(y)- \bar{\lambda}^T(Ax+By-c)$ 
    satisfies the KL property with exponent $\alpha$ at an $(\bar{x},\bar{y}) \in \Argmin F$ where $\bar{\lambda}$ is the Lagrangian multiplier of problem \eqref{problem}  
    and $\emptyset \neq \Argmin F_{\bar{\lambda}}\cap \mathcal{D}$. It hods that there exist $c>0,\eta>0$ and
    a neighborhood $U$ of $(\bar{x}, \bar{y})$ such that for every $(x,y)\in U$,  
    \begin{align} 
        \mathrm{dist}\left((x,y), \Argmin F\right) \leq c\left(f(x) + g(y)-f(\bar{x})-g(\bar{y})+\eta\|Ax+By-c\rVert\right)^{1-\alpha} \nonumber
    \end{align} 
\end{theorem}

    \begin{proof}
        Let $\mathcal{C} = \Xi \times \Delta$. Due to \cite[Corollary 3]{bauschke1999strong}, there exist a $\kappa>0$ such that
        \begin{align} 
            \mathrm{dist}((x,y),\mathcal{C}\cap \mathcal{D}) \leq \kappa \mathrm{dist}((x,y),\mathcal{D}) 
            \label{corollary_bauschke}
        \end{align}
    
        Next, since $F_{\bar{\lambda}}$ satisfies the KL property with exponent $\alpha$ at $(\bar{x},\bar{y})$, the 
        function $F$ also satisfies the KL property with exponent $\alpha$ thanks to Lemma \ref{lemma_of_Lagrangian_KL_exponent}. Thus, the following
        result holds due to Lemma \ref{error_bound}.
        \begin{align}
            \mathrm{dist} ((x,y),\Argmin F) \leq \bar{c}(F(x,y) - F(\bar{x},\bar{y}))^{1-\alpha} \label{property_of_error_bound}
        \end{align}
        for some $\bar{c} >0$, $\epsilon>0$, $r_0>0$ and $(x,y)$ satisfying $\mathrm{dist}((x,y),(\bar{x},\bar{y})) \leq \epsilon\leq 1$ and 
        $F(\bar{x},\bar{y})<F(x,y)<F(\bar{x},\bar{y}) + r_0$.
        Besides, since $f_0(x)+ g_0(y)$ is convex and hence it is locally Lipschitz continous. 
        Therefore, there exist $\epsilon_1$ and $L_0$ such that if $\|(x,y) - (\bar{x},\bar{y})\rVert \leq \epsilon_1$, 
        $|f_0(x)+g_0(y) - f_0(\bar{x}) -g_0(\bar{y})| \leq L_0\|(x,y) - (\bar{x},\bar{y})\rVert $. 
        Let $(\tilde{x},\tilde{y})= P_{\mathcal{C}\cap \mathcal{D}}((x,y))$ which denote the projection of $(x,y)$ onto $\mathcal{C}\cap \mathcal{D}$.
        Let $\epsilon_2 = \min\{\epsilon, \epsilon_1\}$. Therefore, if $\mathrm{dist}((x,y),(\bar{x},\bar{y}))\leq \epsilon_2$, we have that 
        \begin{align}
            &\mathrm{dist}((x,y), \Argmin F) \leq \mathrm{dist}(P_{\mathcal{C}\cap\mathcal{D}}((x,y)), \Argmin F)
            + \mathrm{dist}((x,y),\mathcal{C}\cap\mathcal{D}) \nonumber \\
            & \overset{\mathop{(a)}}{\leq} \bar{c}(F(P_{\mathcal{C}\cap \mathcal{D}}(x,y)) - F(\bar{x},\bar{y}))^{1-\alpha} + \kappa\mathrm{dist}((x,y),\mathcal{D}) \nonumber \\
            & =\bar{c}(f_0(\tilde{x})+g_0(\tilde{y})-f_0(\bar{x})-g_0(\bar{y}))^{1-\alpha} + \kappa\mathrm{dist}((x,y),\mathcal{D})  \nonumber \\
            & \overset{\mathop{(b)}}{\leq} \bar{c}(f_0(x)+g_0(y)-f_0(\bar{x})-g_0(\bar{y})+L_0\mathrm{dist}((x,y), \mathcal{C}\cap\mathcal{D})) +\kappa \mathrm{dist}((x,y),\mathcal{D}) \nonumber \\
            & \overset{\mathop{(c)}}{\leq} \bar{c}(f_0(x)+g_0(y)-f_0(\bar{x})-g_0(\bar{y})+L_0\mathrm{dist}((x,y), \mathcal{C}\cap\mathcal{D}))^{1-\alpha}
            + \kappa \mathrm{dist}((x,y),\mathcal{C}\cap \mathcal{D})^{1-\alpha} \nonumber \\
            &\leq c(f_0(x)+g_0(y)-f_0(\bar{x})-g_0(\bar{y})+ \kappa_1\mathrm{dist}((x,y),\mathcal{D}))^{1-\alpha} \nonumber \\
            &\leq c(f_0(x)+g_0(y)-f_0(\bar{x})-g_0(\bar{y})+ \kappa_1\tilde{c}\|Ax+ By-c\rVert)^{1-\alpha} 
            \label{error_bound_f+g}
        \end{align} 
        where $(a)$ holds because \eqref{corollary_bauschke} and \eqref{property_of_error_bound}, $(b)$ holds because $f_0(x)+g_0(y)$ is locally Lipschitz and $\epsilon_2 = \min\{\epsilon, \epsilon_1\}$ 
        $(c)$ holds because $\mathrm{dist}((x,y),\mathcal{C}\cap \mathcal{D}) \leq 1$ and the last inequality holds because 
        $a^{1-\alpha}+ b^{1-\alpha} \leq 2^{\alpha} (a+b)^{1-\alpha}$, $c = 2^{\alpha}\bar{c}$, $\kappa_1 = L_0\kappa+\kappa^{\frac{1}{1-\alpha}}$. 
        Besides, the last inequality holds because of \cite[Lemma 3.2.3]{facchinei2003finite}. We finally 
        obtain the desired result by letting $\eta = \kappa_1\tilde{c}$. 
    \end{proof}

    Now, based on the Theorem \ref{Theorem_of_KL}, we can establish the convergence rate to the solution set 
    of problem \ref{problem} when $\{x^k,y^k\}$ is approaching the solution set. 
    \begin{corollary}
        Consider the problem \ref{problem}. Suppose that the Assumption \ref{Assumption 1}, \ref{Assumption 2} 
        hold, $\bar{\lambda}$ is the Lagrangian multiplier of problem \ref{problem} and $(\bar{x}, \bar{y})$ is a 
        solution of problem \ref{problem}. Furthermore, we suppose that 
        the functions $x \mapsto f(x) -\bar{\lambda}^TAx$ and $y\mapsto g(y)-\bar{\lambda}By$ satisfy the KL property 
        with exponent $\alpha$. 
        Define $F = f(x)+ g(y)+ \delta_{\mathcal{D}}(x,y)$ where $\mathcal{D}=\{(x,y)|Ax+By=c\}$. 
        Then, there exist $K>0$ and $\eta>0$ such that for all $k>K$, we have that   
        \begin{align}
            \mathrm{dist}\left((x^k,y^k), \Argmin F\right) \leq  c(\max \{\tau_k, \|\bar{\lambda}\rVert\gamma_k\}+\eta\gamma_k)^{1-\alpha}   \nonumber
        \end{align}
        where $\tau_k$ is defined in Theorem \ref{theorem_f+beta_h} and 
        $\gamma_k = \frac{\|\bar{\lambda}\rVert^2}{\beta_k} + \|\bar{\lambda}\rVert\sqrt{\frac{\|\bar{\lambda}\rVert^2}{\beta_k^2}+\frac{2\tau_k}{\beta_k}}$ 
        where $\beta_k = (k+1)^{\delta}$. 
    \end{corollary}
    \begin{proof}
        First, recall that the functions $x\mapsto f(x)-\bar{\lambda}^TAx$ and $y\mapsto g(y)-\bar{\lambda}^TBy$ 
        satisfy the KL property with exponent $\alpha$. Due to \cite[Theorem 3.3]{li2018calculus}, we obtain 
        that $F_{\bar{\lambda}}(x,y):= f(x)+g(y)-\bar{\lambda}^T(Ax+ By-c)$ also satisfies the 
        KL property with the exponent $\alpha$ at $(\bar{x}, \bar{y})$.  

        Second, because of Theorem \ref{Theorem_of_KL}, there exist $0<\epsilon<1$, $c>0$, $\eta>0$, $r_0 \in (0,\infty]$ such that 
        for any $(x,y)$ satisfying $\mathrm{dist}((x,y),(\bar{x},\bar{y}))<\epsilon$ and 
        $F(\bar{x},\bar{y})<F(x,y)<F(\bar{x},\bar{y})+ r_0$, it holds that 
        \begin{align}
            \mathrm{dist}\left((x^k,y^k), \Argmin F\right) &\leq  c\left(f(x^k) + g(y^k)-f(\bar{x})-g(\bar{y})+\eta\|Ax^k+By^k-c\rVert\right)^{1-\alpha} \nonumber \\
            & \leq c(\max \{\tau_k, \|\bar{\lambda}\rVert\gamma_k\}+\eta\gamma_k)^{1-\alpha} \nonumber
        \end{align}
        Finally, thanks to \cite[Lemma 3.2.3]{facchinei2003finite}, we obtain that there exists a $\tilde{c}$ 
        such that $\mathrm{dist}((x^k,y^k),\Argmin F )\leq \tilde{c}\|Ax^k+By^k -c\rVert$. Besides, 
        Recalling theorem \ref{theorem_f+beta_h} and the definition of $\tau_k$ therein, we have that 
        $\mathrm{dist}((x^k,y^k),\Argmin F )\leq \tilde{c} \gamma_k$. Therefore, we choose $K$ such that 
        $\tilde{c}\gamma_K < \epsilon$. The desired result follows. 
    \end{proof}

    % \begin{example}
    %     Suppose that $\mathcal{A}: \mathbb{R}^{m\times n}\rightarrow \mathbb{R}^p$, 
    %     $\mathcal{B}: \mathbb{R}^{m\times n} \rightarrow \mathbb{R}^{m_1\times n_1}$ are linear maps, 
    %     $X\in \mathbb{R}^{m\times n}$ and $b\in \mathbb{R}^p$. If $\bar{\lambda}$ is the Lagrangian multiplier of 
    %     the following problem
    %     \begin{align} 
    %         \min\limits \hspace{4pt}&\frac{1}{2}\|\mathcal{A}X-b\rVert^2 \nonumber\\
    %         s.t. \hspace{4pt} &\| \mathcal{B}X \rVert_* \leq \sigma   \nonumber
    %     \end{align}
    %     The Lagrangian function of this problem is $F_{\bar{\lambda}}(X) = \frac{1}{2}\|\mathcal{A}X-b\rVert^2
    %     +\bar{\lambda}(\|\mathcal{B}X\rVert_*-\sigma)$. Furthermore, suppose that 
    %     $0 \in \mathrm{ri} \hspace{2pt} \partial F_{\bar{\lambda}}$. We obtain that the KL 
    %     exponent of $F_{\bar{\lambda}}$ is $\frac{1}{2}$. 
    % \end{example}
    \begin{example}
        Consider the following problem
        \begin{equation}\label{example_lasso}
            \begin{array}{cl}
            \min\limits_{x\in \mathbb{R}^n} & \|Ax-b\|_1\\
            {\rm s.t.} & \kappa_P(x)\le c_1.
            \end{array}
        \end{equation}
        where $\kappa_P(x)$ is defined as follows
        \begin{equation*} 
            \kappa_{\mathcal{P}}(x):=\min_{s_k\in \mathbb{R}^n}\left\{\sum_{k=1}^K\|s_k\|^2:\; x = \sum_{k=1}^{K} s_k, (s_k)_i=0, \forall i\notin G_k \right\}.
        \end{equation*}
        Let $s = (s_1, s_2,\dots, s_k)$, $F_{\bar{\lambda}}(x,s) =\|Ax-b\|_1+\bar{\lambda} \left(\sum_{k=1}^K \|s_k\|^2 + \delta_{\{x\}}\left(s\cdot\mathbbm{1}_n\right)
        + \sum_{k=1}^K\delta_{\Omega_k}(s_k)-c \right)$, where $\mathbbm{1}_n = (1,1,\cdots,1)\in \mathbb{R}^n$ and 
        $\Omega_k = \left\{ s_k| (s_k)_i = 0,\forall i\notin G_k \right\}$. By \cite[Theorem 3.1]{yu2022kurdyka},  we obtain that 
        the function $F_{\bar{\lambda}}(x) = \|Ax-b\|_1 + \bar{\lambda}(\kappa_P(x)-c_1)$ also satisfies the KL 
        property with exponent $\frac{1}{2}$ at every solution to problem \eqref{example_lasso} .
    \end{example}
    \begin{example}
        Consider the problem
        \begin{equation}\label{KL_hp}
            \begin{array}{cl}
            \min\limits_{y} & \langle u, y\rangle \\
            {\rm s.t.} & \|y\|_\infty \le 1,\\
            & y\in \cal{K}^*.
            \end{array}
            \end{equation}
            where $\cal{K}$ is the hyperbolicity cone and $\cal{K}^*$ is the dual cone of $\cal{K}$. 
            Notice that the functions $x \mapsto \delta_{\|x\|_{\infty}\leq 1}(x)$ and $x\mapsto \delta_{\cal{K}^*}(x)$ 
            are semialgebraic.    
    \end{example}





\section{Examples}
\subsection{Hankel Matrix Rank Minimization}

Consider the following problem
\begin{align} \label{Hankel Matrix}
    \min\limits \hspace{4pt}&\frac{1}{2}\|\mathcal{A}(x)-b\rVert^2 \nonumber\\
    s.t. \hspace{4pt} &\| \mathcal{H}(x) \rVert_* \leq \sigma   
\end{align}
where $\mathcal{A}: \mathbb{R}^{m\times n(k+j-1)} \rightarrow \mathbb{R}^p$ is a linear operator,   
$y =\left(x_0, x_1,...,x_{j+k-2}\right)$ is an $m \times n(k+j-1)$ matrix and each
$x_i$ is an $m\times n$ matrix. In this paper, $\mathcal{A}(x)$ is the operator that misses some elements
$x_i$ in $x$ and then stretches it into a vector.  Therefore, we can obtain that
\begin{align}
    \lambda_{\max}(\mathcal{A}^*\mathcal{A}) \leq 1 \nonumber
\end{align}
Besides, $\mathcal{H}(x) := H_{m, n, j, k}(x)U$, where
\begin{align}
    H_{m,n,j,k}(y) = \left(
        \begin{array}{cccc}
            x_0 & x_1 & \cdots &x_{k-1} \\
            x_1 & x_2 & \cdots &x_{k} \\
            \vdots & \vdots &  &\vdots \\
            x_{j-1} &x_{j}  &  &x_{j+k-2} 
        \end{array} \nonumber
    \right) \in \mathbb{R}^{mj\times nk}
\end{align}
and
$U$ is a matrix whose columns are orthogonal. Therefore, We can also obtain that 
\begin{align}
    \| \mathcal{H}^*\mathcal{H}(x) \rVert \leq \mathbf{r}\| x\rVert \nonumber
\end{align}
where $\mathbf{r} = \min\{ j,k\}$.

Because $\|\mathcal{H}(x)\rVert_* \leq \sigma$, we have that
\begin{align}
    tr\left(\mathcal{H}(x)^T\mathcal{H}(x)\right) \leq \sigma^2 \nonumber
\end{align}
Notice that 
\begin{align}
    &tr\left(\mathcal{H}(x)^T\mathcal{H}(x)\right)  = 
    tr\left(H_{m,n,j,k}^T H_{m,n,j,k}\right) =\sum\limits_{i =0}^{k-1}\sum\limits_{l=i}^{l+j-1} 
    tr\left(x_l^Tx_l\right) \geq \sum\limits_{i=0}^{k+j-2} tr\left(x_i^Tx_i\right)
    = tr\left(x^Tx\right) \nonumber
\end{align}
Therefore, we have that $\|x\rVert_F \leq \sigma$.
Then the problem \eqref{Hankel Matrix} can be written as 
\begin{align}\label{Hankel_Matrix_reformulation}
    \min\hspace{4pt} f(x) + g(y) +\frac{r_k}{2}\|y-\mathcal{H}(x) \rVert_F^2 
\end{align}
where $f(x) = \frac{1}{2}\|\mathcal{A}(x)-b\rVert^2 + \delta_{\|\cdot\rVert_F \leq \sigma}(x)$, 
$g(y) = \delta_{\|\cdot\rVert_* \leq \sigma}(y), r_k = \sqrt{k}$, $\alpha_k =\frac{2}{k+2}$.
Therefore, we can write the closed form of algorithm $\ref{alg1}$.
First, we can get SVD decomposition of $y^k - \mathcal{H}(y^k)$, i.e.
\begin{align}
    y^k -\mathcal{H}(x^k) = U\Sigma V^T  = U \left(\begin{array}{cccc}
        \sigma_1 &  & & \\
          &\sigma_2 & & \\
          &   & \ddots & \\
          &   &     &\sigma_s
    \end{array}\right)  V^T \nonumber
\end{align}
Suppose that $i_0$ is the index of the maximum element of $\Sigma$, $U_{i_0}$ is the $i_0$-th row of $U$ and 
$V^T_{i_0}$ is the $i_0$-th column of $V^T$. 
In the Frank-Wolfe step, 
we need  to solve the following subproblem
\begin{align}
    \min\limits_{\|y\rVert_* \leq \sigma} \hspace{4pt} \langle y^k - \mathcal{H}(x^k), y\rangle \nonumber
\end{align}
whose solution is $\sigma_{i_0}U_{i_0}V_{i_0}^T$. Therefore, the algorithm $\ref{alg1}$ for solving 
problem $\eqref{Hankel_Matrix_reformulation}$ can be written as follows. 
\begin{align}
    \left\{
        \begin{aligned}
            x^{k+1} &= \mathrm{Prox}_{\delta_{\|\cdot\rVert_F \leq \sigma}}\left(x^k-\frac{1}{1+\sqrt{k}\mathbf{r}}\left(\mathcal{A^*}\left(\mathcal{A}(x^k)-b\right)
            +\mathcal{H^*}\left(\mathcal{H}(x^k) -y^k\right)\right)\right) \\
            u^k &= -\sigma_{i_0}U_{i_0}V_{i_0}^T \\
            y^{k+1} &= y^k+ \alpha_k(u^k-y^k)
        \end{aligned}
     \right.
\end{align}

\subsection{The $ \ell_1 $ hyperbolicity projection}
Consider 
\begin{equation}\label{l1_hp}
\begin{array}{cl}
\min\limits_x & \|x-u\|_1\\
{\rm s.t.} &  x\in \cal{K}.
\end{array}
\end{equation}


The dual problem of \eqref{l1_hp} is 
\begin{equation}\label{dual_hp}
\begin{array}{cl}
\min\limits_{y} & \langle u, y\rangle \\
{\rm s.t.} & \|y\|_\infty \le 1,\\
& y\in \cal{K}^*.
\end{array}
\end{equation}

With an additional variable and a linear equality constraint introduced, we solve
\begin{equation}\label{equiv_hp}
\begin{array}{cl}
\min\limits_{y,\theta} & \langle u, y\rangle \\
{\rm s.t.} & \|y\|_\infty \le 1, \quad y - \theta=0, \\
& \theta\in \mathcal{K}^* ,\quad \langle e, \theta \rangle \leq \|e\|_1 .
\end{array}
\end{equation}

We apply the alternating updating scheme to 
\begin{equation*}
\begin{array}{cl}
\min\limits_{y,\theta} & \langle y, u\rangle +\frac{\beta_{t}}{2}\|y-\theta\|^2\\
{\rm s.t.} &  \|y\|_\infty \le 1, \quad \\
& \langle e, \theta\rangle \le \|e\|_1, \quad \theta\in \cal{K}^*.
\end{array}
\end{equation*}
We say that $ f(y)=\langle u, y\rangle +\delta_{\|\cdot\|_\infty\le 1} $, $ g(\theta) = \delta_{\mathcal{K}^*\cap \{\theta:~\langle e, \theta\rangle \le \|e\|_1 \} }(\theta) $ and $ h(y,\theta) = \frac{\beta_t}{2}\|y-\theta\|^2 $.

\begin{equation}\label{alg_steps}
\left\{  
\begin{array}{l}
y^t  = \min\{\max\{ \theta_t-u/\beta_t, -\boldmath{1} \} , \boldmath{1} \},\\
\theta^t \in\arg\min\{\langle \beta_t(\theta^t-y^{t+1}), \theta\rangle:~ \langle e, \theta\rangle\le \|e\|_1, ~\theta\in \cal{K}^* \}.
\end{array}
\right.
\end{equation}

%\begin{equation*}
%	\begin{array}{cl}
%	\min\limits_{x} & \|x-u\|_1\\
%	{\rm s.t.} & x\in \mathbb{R}^{n, (k)}_+.
%	\end{array}
%\end{equation*}

%\begin{table}
%	\begin{tabular}{c  c   c}
%		\hline 
%		Error & Mean $\pm$stdVar & Success Rate \\
%		10\% &	  6.02 $\pm$	  4.12 &			100.0 \\	      	
%		5\%&	 16.94 $\pm$	 15.74 &			100.0 \\ 		
%		1\%&	 46.76 $\pm$	 33.69 &			53.3  \\ 			
%		0.5\%&	 45.24 $\pm$	 31.62 &			30.0 \\ 			
%		0.1\%&	 21.94 $\pm$	  9.27 &	 		6.7 \\			
%		0.05\%&	 23.21 $\pm$	  7.48 &	 		6.7 \\	 			
%		0.01\%&	 33.70 $\pm$	 22.32 &	 		6.7 \\	 			
%		0.005\%&	 33.70 $\pm$	 22.32 &	 		6.7 \\	 			
%		0.001\%&	 33.70 $\pm$	 22.32 &	 		6.7 \\ 	\hline
%	\end{tabular}
%\end{table}


%\subsection{The latent group Lasso}
%
%Given a set of overlapping groups $ \{G_1, \cdots, G_K\}$ satisfying $\bigcup\limits_{k=1}^K = \{1, \cdots, n\} $, the latent group norm \cite{obozinski2011group} is define as 
%\begin{equation*}
%	\kappa_{\mathcal{P}}(x):=\min_{s_k\in \mathbb{R}^n}\left\{\sum_{k=1}^K\|s_k\|^2:\; x = \sum_{k=1}^{K} s_k, (s_k)_i=0, \forall i\notin G_k \right\}
%\end{equation*}
%The dual norm of $ \kappa_{\mathcal{P}}(x) $  \cite[Lemma~3]{obozinski2011group} is 
%\begin{equation*}
%	\kappa^*_{\mathcal{P}}(z) = \max\limits_{k=1,\cdots, K}\|z_{G_k}\|,\; \forall z\in\mathbb{R}^n.
%\end{equation*}
%
%%Also, $ \kappa_{\mathcal{P}}(x) $ is a gauge function 
%%\[ \kappa_{\mathcal{P}}(x) = \min\{ \mu\geq 0: x\in \mathcal{P} \}\]
%%with the closed convex set 
%%\[
%% \mathcal{P}:={\rm conv}\left( \bigcup\limits_{k=1}^k\{ s_k\in \mathbb{R}:\; \|s_k\|\le 1, (s_k)_i=0, \forall i\notin G_k \} \right).
%%\]
%
%Consider the latent group Lasso problem
%\begin{equation}
%	\begin{array}{cl}
%	\min\limits_{x\in \mathbb{R}^n} & \|Ax-b\|_1\\
%	{\rm s.t.} & \kappa_P(x)\le c_1.
%	\end{array}
%\end{equation}
%We rewrite it as 
%\begin{equation}
%\begin{array}{cl}
%\min\limits_{x\in \mathbb{R}^n} & \|y\|_1\\
%{\rm s.t.} & y = Ax-b, \; \kappa_P(x)\le c_1, \; \|y\|\le c_2
%\end{array}
%\end{equation}
%We solve the problem with the following iterative scheme
%\begin{equation}
%	\left\{
%	\begin{array}{l}
%	y^{t+1} \in \arg\min \left\{\|y\|_1 +\dfrac{\beta_t}{2}\|y-(Ax^t-b)\|^2:\; \|y\|\le c_2 \right\},\\
%	u^t \in \arg\min \left\{\langle\beta_tA^T(Ax^t-y^{t+1}-b), x\rangle :\;\kappa_{\mathcal{P}}(x)\le c_1 \right\},\\
%	x^{t+1} = x^t+\dfrac{2}{t+2}(u^t-x^t).
%	\end{array}
%	\right.
%\end{equation}
%Let 
%\[
%\widetilde{y} = {\rm sgn}(Ax^t-b)\circ\max\{Ax^t-b - (1/{\beta_t})e, 0\}.
%\]
%From the first subproblem, we have
%\[
%y^{t+1} = \min\{c_2/\|\widetilde y\|, 1\}\widetilde{y}.
%\]
%
%Let $ z^t = \beta_tA^T(Ax^t-y^{t+1}-b) $. The second subproblem above computes the dual norm $ \kappa^*_{\mathcal{P}}(z^t) $ of the latent group norm \cite[Example~16]{sun2022screening}. It then holds that $ -z^t\in \partial \delta_{\kappa_{\mathcal{P}}(x)\le c_1}(u^t)  $ and we have
%\[
%u^t\in c_1\partial \kappa^*_{\mathcal{P}}(-z^t).
%\] 
%As in \cite[Section~3.1]{rao2013conditional}, we can set
%\[
%u^t_{G_{k_{\rm max}}}=-\dfrac{c_1z^t_{G_{k_{\rm max}}}}{\|z^t_{G_{k_{\rm max}}}\|}; \; u^t_i = 0, \; i\notin G_{k_{\rm max}}.
%\]

\subsection{Sparse latent group Lasso}

%The dual norm of $ \kappa_{\mathcal{P}}(x) $  \cite[Lemma~3]{obozinski2011group} is of the form
%\begin{equation*}
%\kappa^*_{\mathcal{P}}(z) = \max\limits_{k=1,\cdots, K}\|z_{G_k}\|,\; \forall z\in\mathbb{R}^n.
%\end{equation*}

In this section, we consider the sparse multi-task learning (sMTL) problem \cite{rao2013sparse} that enjoys sparsity both in feature group selection and features learning \cite{rao2013sparse}.  The sMTL problem is to find a similar underlying sparse representation shared across related tasks. Conceptually, all tasks first select from few subsets (groups) of the possible features and then every task determine a sparse representation on features from the selected groups. To characterize the structural group sparsity in sMTL, one can use the latent group norm regularizer defined in \cite{obozinski2011group, rao2013sparse}. 

Given the overlapping groups $ \{G_1, \cdots, G_K\}$ satisfying $\bigcup\limits_{k=1}^K = \{1, \cdots, n\} $, the latent group norm  \cite{obozinski2011group} of $ x\in\mathbb{R}^n $ is define as 
\begin{equation*}
\kappa_{\mathcal{P}}(x):=\min_{s_k\in \mathbb{R}^n}\left\{\sum_{k=1}^K\|s_k\|^2:\; x = \sum_{k=1}^{K} s_k, (s_k)_i=0, \forall i\notin G_k \right\}.
\end{equation*}
Compared to the group norm $ \sum_{k=1}^{K}\|x_{G_k}\| $, the latent group norm could induce nonzero support at the overlapping zones of selected and unselected groups. We refer the readers to \cite[Figure~1]{obozinski2011group} for a more intuitive comprehension on the comparison.

In our numerical experiments, we generate some synthetic data of sMTL problems. Suppose that there are  $ \mathcal{T}=20 $ tasks and $ N=2002 $ features. The $ N $ features are grouped into $ K=500 $ overlapping groups $ G_1=\{1,2,3,4,5,6\} $, $ G_2=\{5,6,7,8,9,10\} $, where each group has batch size $ B=6 $ and every next two groups has $ 2 $ overlapping features. Among those groups, only $ s_g\ll K $ groups are active for all tasks. For each $ t=1,\cdots,  \mathcal{T}$ task, we set $ m=500 $ measurements. We randomly generate $  \mathcal{T} $ matrices $ M_1,\cdots, M_ \mathcal{T}\in \mathbb{R}^{m\times N} $ with i.i.d Gaussian entries. For every task $ t=1,\cdots,  \mathcal{T}$, we form the data matrix $ A_t\in\mathbb{R}^{m\times N} $ by normalizing the columns of $ A_t $. Suppose that $ x_t\in\mathbb{R}^N $ is the true parameter vector for task $ t $.  We then set the observation vector of task $ t $ as $ b_t = A_t x+ 0.01\epsilon $, where $ \epsilon $ is the Gaussian noise. 

For convenience, we denote $ A\in\mathbb{R}^{m \mathcal{T}\times N \mathcal{T}} $ the block diagonal matrix whose diagonals are $ A_t $, $ t=1,\cdots, \mathcal{T} $. Let $ x=[x_1^T, x_2^T,\cdots, x_ \mathcal{T}^T]^T\in \mathbb{R}^{N\mathcal{T}} $ and $ b = [b_1^T,b_2^T,\cdots,b_\mathcal{T}^T]^T\in \mathbb{m\mathcal{T}} $. We formulate the multi-task learning problem as a sparse latent group Lasso problem as in \eqref{sparselgl}, where the $ \ell_1 $ regularizer is for the element-wise sparsity and the latent group norm $ \kappa_{\mathcal{P}} $ is for the group sparsity. 
\begin{equation}\label{sparselgl}
	\begin{array}{cl}
	\min\limits_{x} & \frac12\|Ax-b\|^2 +\mu \|x\|_1\\
	{\rm s.t.} & \kappa_{\mathcal{P}}(x)\leq c_1.
	\end{array}
\end{equation}

Note that there is no closed-form solution for the projection onto the latent group norm ball\cite{rao2013conditional}. Instead of projections, we apply Algorithm~\ref{alg1} to solve the MTL problem that involves a linear oracle with closed-form solution defined over the latent group norm ball. To adapt the algorithm, we introduce an additional variable $ y $ and write \eqref{sparselgl} as the following equivalent formulation:
\begin{equation}
	\begin{array}{cl}
	\min\limits_{x,y} & \frac12\|Ax-b\|^2 +\mu \|x\|_1\\
	{\rm s.t.} & \|x\|\le c_1,\; \kappa_{\mathcal{P}}(y)\leq c_1, \; y=x.
	\end{array}
\end{equation}
Let $ f(x) = \frac{1}{2}\|Ax-b\|^2 + \mu\|x\|_1+\delta_{\|\cdot\|_1\le c_1}(x) $, $ g(y) = \delta_{\kappa_{\mathcal{P}}\le c_1}(y) $ and $ h(x,y) = \frac{1}2\|x-y\|^2$ in \eqref{problem_2}. The subproblems for updating $ x $ and $ y $ of the iterative scheme in Algorithm~\ref{alg1} then solves:
%\begin{equation}
%	\begin{array}{cl}
%	\min\limits_{x,y}  & \frac12\|Ax-b\|^2 +\mu \|x\|_1 + \frac{\beta}{2}\|x-y\|^2\\
%		{\rm s.t.} & \|x\|\le c_1,\; \kappa_{\mathcal{P}}(y)\leq c_1.
%	\end{array}
%\end{equation}
\begin{equation}
\left\{
\begin{array}{l}
x^{t+1} \in \arg\min\limits_{ \|x\|\le c_1} \mu\|x\|_1 + \langle A^T(Ax^t-b)+\beta_t(x^t-y^t), x\rangle+\dfrac{\beta_t+L_A}{2}\|x - x^t\|^2\\
u^t \in \arg\min \left\{\langle\beta_t(y^t - x^{t+1}), x\rangle :\;\kappa_{\mathcal{P}}(y)\le c_1 \right\},\\
y^{t+1} = y^t+\dfrac{2}{t+2}(u^t-y^t).
\end{array}
\right.
\end{equation}
Here, $ L_A $ is an upper bound on the largest eigenvalue of $ A^TA $.
Both the subproblems above allow closed-form solutions. Let $ g^t = \dfrac{1}{L_A+\beta_t}(L_Ax^t+\beta_t y^t - A^T(Ax-b))  $  $z^t = \beta_t(y^t-x^{t+1}) $. The subproblem for $ u $ above actually computes the dual norm of the latent norm of $ -z^t $ \cite{obozinski2011group,sun2022screening}. Specifically, we can choose
\begin{equation}
\left\{
\begin{array}{l}
x^{t+1} = {\rm Proj}_{\|\cdot\|\le c_1}({\rm Prox}_{\frac{\mu}{L_A+\beta_t}}(g^t) )\\
u^t = -\dfrac{c_1z_{G_{\rm max}}}{\|z_{G_{\rm max}}\|},
\end{array}
\right.
\end{equation} 
where $ G_{\rm max}\in \arg\max\{\|z_{G_k}\|: \, k=1,\cdots, K \} $. Here we choose $ u^t $ as in \cite[Part ~B, Section~IV]{rao2013sparse}.
 




%Consider the overlapping group Lasso problem
%\begin{equation}
%	\begin{array}{cl}
%	\min\limits_{x\in \mathbb{R}^n} &\|Ax-b\|_1\\
%	{\rm s.t.} & \sum\limits_{i=1}^I \|x_{G_i}\|\le \sigma,  \quad \bigcup\limits_{i=1}^I \{G_i\}=\{1, \cdots, n\}.
%	\end{array}
%\end{equation}
%
%Let $ G_0 = \emptyset $ and set $ |G_0|=0 $. Denote by $ N = \sum_{i=0}^I |G_i|$. We rewrite the group Lasso problems as
%\begin{equation}\label{gl-xy}
%	\begin{array}{cl}
%	\min\limits_{x\in\mathbb{R}^n, y\in \mathbb{R}^N, z\in\mathbb{R}^m}  & \|z\|_1\\
%	{\rm s.t.} & z = Ax-b ,\;\; \sum \limits_{i=1}^I \|y_{[|G_i|+1:|G_{i+1}|]}\| \le \sigma, \\
%	& x_{G_i} = y_{[|G_i|+1:|G_{i+1}|]}, \;\; i=0,1,\cdots, I-1.
%	\end{array}
%\end{equation}
%
%For convenience, we write the last $ I $ linear constraint in \eqref{gl-xy} as $ My = x $ with some matrix $ M\in \mathbb{R}^{n\times N} $, where $ n< N $. Now we solve the penalized problem of \eqref{gl-xy} as follows:
%\begin{equation*}
%	\begin{array}{cl}
%	\min\limits_{x\in\mathbb{R}^n, y\in \mathbb{R}^N, z\in \mathbb{R}^m}  & \|z\|_1 + \dfrac{\beta}{2}\|Ax-z-b\|^2+ \dfrac{\beta}{2}\|Mx-y\|^2 \\
%	{\rm s.t.} & \sum \limits_{i=1}^I \|y_{\left[|G_i|+1:|G_{i+1}|\right]}\| \le \sigma, \;\;\|x\|\le \sigma\;\; \|z\|\le \widetilde{z}.
%	\end{array}
%\end{equation*}
%
%We use the algorithm 
%\begin{equation}\label{alg_steps_gl}
%\left\{  
%\begin{array}{l}
%%x^t \in \arg\min \left\{\langle \nabla f(x^t)+\beta_tM^T(Mx^t-y^t), x\rangle +\dfrac{L_f+\beta_t\sqrt{\lambda_{\rm max}(M^TM)}}{2}\|x-x^t\|^2:~ \|x\|\le \sigma\right\},\\
%z^t\in \arg\min \left\{ \|z\|_1+\langle z^t-Ax^t+b, z\rangle +\dfrac{\beta_t}{2}\|z-z^t\|^2:~\|z\|\le \widetilde\sigma  \right\} \\
%x^t \in \arg\min \left\{ 
% \langle\beta_t A^T(Ax^t-z^t-b)+ M^T(Mx^t-y^t), x\rangle +\dfrac{\beta_t(L_A+L_M)}{2}\|x-x^t\|: ~\|x\|\le \sigma, \right\} \\
%y^t \in\arg\min \left\{\langle \beta_t(y^t-Mx^{t+1}), y\rangle:~  \sum \limits_{i=1}^I \|y_{\left[|G_i|+1:|G_{i+1}|\right]}\| \le \sigma \right\}.
%\end{array}
%\right.
%\end{equation}



\bibliography{sample}


\end{document}